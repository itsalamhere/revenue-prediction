{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe61290",
   "metadata": {},
   "source": [
    "# Revenue Prediction via CLV by Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc31ec",
   "metadata": {},
   "source": [
    "In the previous section, we've cleaned and wrangled the data for the prediction. Now in this section we will utilize the models of machine learning (ML). The prediction will be implemented with backtesting. Here are the steps: \n",
    "\n",
    "* We will call a function of `get_rfm_features()` for several periods, with base total transaction of six months (`2009-12-01` to `2010-12-01`). In this case, we're predicting the revenue for every 2 months (9-1 = 8 tests in total)\n",
    "\n",
    "* We will save the calculation for each period in a DataFrame, and use it for prediction in the next periods. Keep in mind that the features we're using are the calculation of RFM, which can change for each period. \n",
    "\n",
    "* We'll calculate the accuracy by comparing the real and predicted target revenue in each periods, and lastly we'll average it to find the model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e25d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-lego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eb8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6adc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb0f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94324fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score)\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Connecting model for both classification \n",
    "# and regression in zero-inflated dataset\n",
    "from sklego.meta import ZeroInflatedRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f2d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('online_retail_II_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa04159f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Country</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489434</td>\n",
       "      <td>85048</td>\n",
       "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.95</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>83.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323P</td>\n",
       "      <td>PINK CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323W</td>\n",
       "      <td>WHITE CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489434</td>\n",
       "      <td>22041</td>\n",
       "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
       "      <td>48</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>100.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489434</td>\n",
       "      <td>21232</td>\n",
       "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
       "      <td>24</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820417</th>\n",
       "      <td>581587</td>\n",
       "      <td>22613</td>\n",
       "      <td>PACK OF 20 SPACEBOY NAPKINS</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820418</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820419</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820420</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820421</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "      <td>14.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820422 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Invoice StockCode                          Description  Quantity  \\\n",
       "0       489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
       "1       489434    79323P                   PINK CHERRY LIGHTS        12   \n",
       "2       489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
       "3       489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
       "4       489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
       "...        ...       ...                                  ...       ...   \n",
       "820417  581587     22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
       "820418  581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
       "820419  581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
       "820420  581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "820421  581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
       "\n",
       "                InvoiceDate  Price  Customer ID         Country  revenue  \n",
       "0       2009-12-01 07:45:00   6.95        13085  United Kingdom    83.40  \n",
       "1       2009-12-01 07:45:00   6.75        13085  United Kingdom    81.00  \n",
       "2       2009-12-01 07:45:00   6.75        13085  United Kingdom    81.00  \n",
       "3       2009-12-01 07:45:00   2.10        13085  United Kingdom   100.80  \n",
       "4       2009-12-01 07:45:00   1.25        13085  United Kingdom    30.00  \n",
       "...                     ...    ...          ...             ...      ...  \n",
       "820417  2011-12-09 12:50:00   0.85        12680          France    10.20  \n",
       "820418  2011-12-09 12:50:00   2.10        12680          France    12.60  \n",
       "820419  2011-12-09 12:50:00   4.15        12680          France    16.60  \n",
       "820420  2011-12-09 12:50:00   4.15        12680          France    16.60  \n",
       "820421  2011-12-09 12:50:00   4.95        12680          France    14.85  \n",
       "\n",
       "[820422 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa669ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the read format in float numbers\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# Cutting the dates for time features\n",
    "\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "df['date'] = pd.to_datetime(df['InvoiceDate'].dt.date)\n",
    "df['time'] = df['InvoiceDate'].dt.time\n",
    "\n",
    "df['hour'] = df['time'].apply(lambda x: x.hour)\n",
    "df['weekend'] = df['date'].apply(lambda x: x.weekday() in [5, 6])\n",
    "df['dayofweek'] = df['date'].apply(lambda x: x.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9947e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recalling the function from previous section\n",
    "\n",
    "### Some modification: \n",
    "\n",
    "## We'll modify the reset_index() to combine all the data later on\n",
    "\n",
    "# and we're changing the datatypes of date inputs to datetime64[ns]\n",
    "# so that we can add the date with 1 for target_start\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "def get_rfm_features(\n",
    "    df: pd.DataFrame, \n",
    "    feature_start: Union[str, pd.DatetimeIndex], \n",
    "      feature_end: Union[str, pd.DatetimeIndex], \n",
    "     target_start: Union[str, pd.DatetimeIndex], \n",
    "       target_end: Union[str, pd.DatetimeIndex]\n",
    "):\n",
    "    \n",
    "    ## Separating the orders based on the date:\n",
    "    df_features_period = df.loc[(df['date'] >= feature_start) &\n",
    "                                (df['date'] <= feature_end)]\n",
    "    \n",
    "    print(f'''Taking RFM values for features: \\n{feature_start} to {feature_end} -- {(\n",
    "        pd.to_datetime(feature_end) - pd.to_datetime(feature_start)).days} days''')\n",
    "    \n",
    "    print(f'''to predict target revenue: \\n{target_start} to {target_end} -- {(\n",
    "        pd.to_datetime(target_end) - pd.to_datetime(target_start)).days} days''')\n",
    "    \n",
    "    ## Getting the features:\n",
    "    features = df_features_period.groupby('Customer ID').agg(\n",
    "            # new_column_name = ('agg_col', 'agg_func')\n",
    "                      recency = ('date', lambda x: (pd.to_datetime(feature_end) - x.max()).days),\n",
    "                    frequency = ('revenue', lambda x: x.loc[x > 0].count()),\n",
    "#               returns_count = ('revenue', lambda x: x.loc[x < 0].count()),\n",
    "                            T = ('date', lambda x: (x.max() - x.min()).days),\n",
    "               monetary_value = ('revenue', 'sum'),\n",
    "               total_quantity = ('Quantity', lambda x: x.loc[x > 0].sum()),\n",
    "#                total_return = ('Quantity', lambda x: x.loc[x < 0].sum() * -1),\n",
    "         purchase_hour_median = ('hour', lambda x: x.median()),\n",
    "            purchase_hour_dow = ('dayofweek', lambda x: x.median()),\n",
    "        purchase_weekend_prop = ('weekend', 'mean')\n",
    "    ).reset_index()   \n",
    "    \n",
    "    # Filter negative `monetary_value` (and also `avg_basket_value`) to 0\n",
    "    features.loc[features['monetary_value'] < 0, 'monetary_value'] = 0    \n",
    "    \n",
    "    features['avg_basket_value'] = features['monetary_value'] / features['frequency']\n",
    "    features['avg_basket_size']  = features['total_quantity'] / features['frequency']\n",
    "    \n",
    "    ## There's data with `frequency == 0`, so we'll fill in the gaps.\n",
    "    features      = features.replace([-np.inf, np.inf], np.nan)\n",
    "    \n",
    "    ## Make the target data:\n",
    "    target_data = df.loc[(df['date'] >= target_start) &\n",
    "                         (df['date'] <= target_end)]\n",
    "\n",
    "    # Target feature 1: `is_returned`\n",
    "    features['is_returned'] = np.where(\n",
    "        features['Customer ID'].isin(target_data['Customer ID']), 1, 0\n",
    "    )\n",
    "    \n",
    "    # Target feature 2: `target_rev`\n",
    "    target_rev = (target_data.groupby('Customer ID')['revenue']\n",
    "                      .sum().rename('target_rev').reset_index())\n",
    "    \n",
    "    features = features.merge(target_rev, how='left', on='Customer ID').fillna(0)\n",
    "\n",
    "    ## Clean negative values in `target_rev`; \n",
    "    ## Some customers go back after 2 months just to return items.\n",
    "    features.loc[features['target_rev'] < 0, ['is_returned', 'target_rev']] = [0, 0]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca052362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking RFM values for features: \n",
      "2009-12-01 to 2010-06-01 -- 182 days\n",
      "to predict target revenue: \n",
      "2006-12-06 to 2011-06-06 -- 1643 days\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "features = get_rfm_features(\n",
    "    df, '2009-12-01', '2010-06-01', '2006-12-06', '2011-06-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d964447d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>T</th>\n",
       "      <th>monetary_value</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>purchase_hour_median</th>\n",
       "      <th>purchase_hour_dow</th>\n",
       "      <th>purchase_weekend_prop</th>\n",
       "      <th>avg_basket_value</th>\n",
       "      <th>avg_basket_size</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12346</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>28.0500</td>\n",
       "      <td>6</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.6750</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>169.3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12349</td>\n",
       "      <td>33</td>\n",
       "      <td>46</td>\n",
       "      <td>146</td>\n",
       "      <td>1044.3700</td>\n",
       "      <td>473</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>22.7037</td>\n",
       "      <td>10.2826</td>\n",
       "      <td>1</td>\n",
       "      <td>2196.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12355</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>488.2100</td>\n",
       "      <td>303</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>22.1914</td>\n",
       "      <td>13.7727</td>\n",
       "      <td>1</td>\n",
       "      <td>947.6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12358</td>\n",
       "      <td>175</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1429.8300</td>\n",
       "      <td>309</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>84.1076</td>\n",
       "      <td>18.1765</td>\n",
       "      <td>1</td>\n",
       "      <td>2519.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12359</td>\n",
       "      <td>50</td>\n",
       "      <td>66</td>\n",
       "      <td>128</td>\n",
       "      <td>1428.2300</td>\n",
       "      <td>777</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>21.6398</td>\n",
       "      <td>11.7727</td>\n",
       "      <td>1</td>\n",
       "      <td>5929.2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>18281</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>120.3200</td>\n",
       "      <td>92</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.0320</td>\n",
       "      <td>9.2000</td>\n",
       "      <td>1</td>\n",
       "      <td>120.3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>18283</td>\n",
       "      <td>65</td>\n",
       "      <td>132</td>\n",
       "      <td>37</td>\n",
       "      <td>354.4200</td>\n",
       "      <td>173</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.4621</td>\n",
       "      <td>2.6850</td>\n",
       "      <td>1.3106</td>\n",
       "      <td>1</td>\n",
       "      <td>1174.8700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>18285</td>\n",
       "      <td>104</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>377.0000</td>\n",
       "      <td>144</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>34.2727</td>\n",
       "      <td>13.0909</td>\n",
       "      <td>1</td>\n",
       "      <td>377.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>18286</td>\n",
       "      <td>167</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>412.9500</td>\n",
       "      <td>113</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.8094</td>\n",
       "      <td>7.0625</td>\n",
       "      <td>1</td>\n",
       "      <td>1138.4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>18287</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>167</td>\n",
       "      <td>1066.5100</td>\n",
       "      <td>732</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>19.7502</td>\n",
       "      <td>13.5556</td>\n",
       "      <td>1</td>\n",
       "      <td>3055.8900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2734 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Customer ID  recency  frequency    T  monetary_value  total_quantity  \\\n",
       "0           12346       91          6   74         28.0500               6   \n",
       "1           12349       33         46  146       1044.3700             473   \n",
       "2           12355       11         22    0        488.2100             303   \n",
       "3           12358      175         17    0       1429.8300             309   \n",
       "4           12359       50         66  128       1428.2300             777   \n",
       "...           ...      ...        ...  ...             ...             ...   \n",
       "2729        18281       21         10    0        120.3200              92   \n",
       "2730        18283       65        132   37        354.4200             173   \n",
       "2731        18285      104         11    0        377.0000             144   \n",
       "2732        18286      167         16    0        412.9500             113   \n",
       "2733        18287       15         54  167       1066.5100             732   \n",
       "\n",
       "      purchase_hour_median  purchase_hour_dow  purchase_weekend_prop  \\\n",
       "0                  13.0000             1.0000                 0.0000   \n",
       "1                  13.0000             3.0000                 0.0000   \n",
       "2                  11.0000             4.0000                 0.0000   \n",
       "3                   7.0000             1.0000                 0.0000   \n",
       "4                  13.0000             2.0000                 0.4571   \n",
       "...                    ...                ...                    ...   \n",
       "2729               10.0000             1.0000                 0.0000   \n",
       "2730               13.0000             4.0000                 0.4621   \n",
       "2731               10.0000             2.0000                 0.0000   \n",
       "2732               10.0000             2.0000                 0.0000   \n",
       "2733               11.0000             0.0000                 0.0000   \n",
       "\n",
       "      avg_basket_value  avg_basket_size  is_returned  target_rev  \n",
       "0               4.6750           1.0000            1    169.3600  \n",
       "1              22.7037          10.2826            1   2196.9900  \n",
       "2              22.1914          13.7727            1    947.6100  \n",
       "3              84.1076          18.1765            1   2519.0100  \n",
       "4              21.6398          11.7727            1   5929.2900  \n",
       "...                ...              ...          ...         ...  \n",
       "2729           12.0320           9.2000            1    120.3200  \n",
       "2730            2.6850           1.3106            1   1174.8700  \n",
       "2731           34.2727          13.0909            1    377.0000  \n",
       "2732           25.8094           7.0625            1   1138.4300  \n",
       "2733           19.7502          13.5556            1   3055.8900  \n",
       "\n",
       "[2734 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8e479",
   "metadata": {},
   "source": [
    "## Recall: Data Wrangling into Periods for ML Prediction\n",
    "\n",
    "On the previous section `03-feature-engineering`, we've accumulated all the data, with `rfm_period` as an identifier. The data is specified as below:\n",
    "\n",
    "* `rfm_period == 0` means the data's taken on a whole six month (`2009-12-01` to `2010-05-31`, and also the target revenue's calculated within the next two months (`2010-06-01` to `2010-08-01`)\n",
    "\n",
    "* As the `rfm_period` increases, the time will shift every two months, so the RFM value will renew; either from a new transaction or not returning.\n",
    "\n",
    "    * Take `rfm_period == 1`with\n",
    "        * training data of `2010-02-01` to `2010-07-31` and \n",
    "        * target revenue calculated from `2010-08-01` to `2011-10-01`,\n",
    "    * Move on to `rfm_period == 2` with \n",
    "        * training data of `2010-04-01` to `2010-09-30` and \n",
    "        * target revenue calculated from `2010-10-01` to `2010-12-01`,\n",
    "    * and so on until it reaches the end of transaction in `2011-12-09`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e02d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods  = ['2009-12-01', '2010-02-01', '2010-04-01', '2010-06-01', \n",
    "            '2010-08-01', '2010-10-01', '2010-12-01', '2011-02-01', \n",
    "            '2011-04-01', '2011-06-01', '2011-08-01', '2011-10-01', '2011-12-09']\n",
    "            ## Last date (09 instead of 01) being last day of transaction. \n",
    "    \n",
    "def accumulate_data(df, periods):\n",
    "    \n",
    "    periods = pd.to_datetime(periods, format='%Y-%m-%d')\n",
    "\n",
    "    total_data = pd.DataFrame()\n",
    "    \n",
    "    for i, date in zip(range(8), periods):\n",
    "        \n",
    "        rfm_features = get_rfm_features(df, \n",
    "            periods[i]  , periods[i+3] - pd.Timedelta(days=1), \n",
    "            periods[i+3], periods[i+4])\n",
    "        rfm_features['rfm_period'] = i\n",
    "        \n",
    "        total_data = pd.concat([total_data, rfm_features], ignore_index=True, sort=False)\n",
    "\n",
    "    # Adjust the column positions: `rfm_period`, features, label\n",
    "    total_data = total_data[ ['rfm_period'] + \n",
    "        [col for col in total_data.columns if col != 'rfm_period'] ]\n",
    "    \n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b2077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking RFM values for features: \n",
      "2009-12-01 00:00:00 to 2010-05-31 00:00:00 -- 181 days\n",
      "to predict target revenue: \n",
      "2010-06-01 00:00:00 to 2010-08-01 00:00:00 -- 61 days\n",
      "Taking RFM values for features: \n",
      "2010-02-01 00:00:00 to 2010-07-31 00:00:00 -- 180 days\n",
      "to predict target revenue: \n",
      "2010-08-01 00:00:00 to 2010-10-01 00:00:00 -- 61 days\n",
      "Taking RFM values for features: \n",
      "2010-04-01 00:00:00 to 2010-09-30 00:00:00 -- 182 days\n",
      "to predict target revenue: \n",
      "2010-10-01 00:00:00 to 2010-12-01 00:00:00 -- 61 days\n",
      "Taking RFM values for features: \n",
      "2010-06-01 00:00:00 to 2010-11-30 00:00:00 -- 182 days\n",
      "to predict target revenue: \n",
      "2010-12-01 00:00:00 to 2011-02-01 00:00:00 -- 62 days\n",
      "Taking RFM values for features: \n",
      "2010-08-01 00:00:00 to 2011-01-31 00:00:00 -- 183 days\n",
      "to predict target revenue: \n",
      "2011-02-01 00:00:00 to 2011-04-01 00:00:00 -- 59 days\n",
      "Taking RFM values for features: \n",
      "2010-10-01 00:00:00 to 2011-03-31 00:00:00 -- 181 days\n",
      "to predict target revenue: \n",
      "2011-04-01 00:00:00 to 2011-06-01 00:00:00 -- 61 days\n",
      "Taking RFM values for features: \n",
      "2010-12-01 00:00:00 to 2011-05-31 00:00:00 -- 181 days\n",
      "to predict target revenue: \n",
      "2011-06-01 00:00:00 to 2011-08-01 00:00:00 -- 61 days\n",
      "Taking RFM values for features: \n",
      "2011-02-01 00:00:00 to 2011-07-31 00:00:00 -- 180 days\n",
      "to predict target revenue: \n",
      "2011-08-01 00:00:00 to 2011-10-01 00:00:00 -- 61 days\n"
     ]
    }
   ],
   "source": [
    "whole_data = accumulate_data(df, periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc25b178",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>T</th>\n",
       "      <th>monetary_value</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>purchase_hour_median</th>\n",
       "      <th>purchase_hour_dow</th>\n",
       "      <th>purchase_weekend_prop</th>\n",
       "      <th>avg_basket_value</th>\n",
       "      <th>avg_basket_size</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12346</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>28.0500</td>\n",
       "      <td>6</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.6750</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>142.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12349</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>146</td>\n",
       "      <td>1044.3700</td>\n",
       "      <td>473</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>22.7037</td>\n",
       "      <td>10.2826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12355</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>488.2100</td>\n",
       "      <td>303</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>22.1914</td>\n",
       "      <td>13.7727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12358</td>\n",
       "      <td>174</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1429.8300</td>\n",
       "      <td>309</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>84.1076</td>\n",
       "      <td>18.1765</td>\n",
       "      <td>1</td>\n",
       "      <td>268.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12359</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>128</td>\n",
       "      <td>1428.2300</td>\n",
       "      <td>777</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>21.6398</td>\n",
       "      <td>11.7727</td>\n",
       "      <td>1</td>\n",
       "      <td>489.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>20</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>146</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>180.6000</td>\n",
       "      <td>45</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.0600</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>80.8200</td>\n",
       "      <td>54</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11.5457</td>\n",
       "      <td>7.7143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>17</td>\n",
       "      <td>297</td>\n",
       "      <td>136</td>\n",
       "      <td>768.8200</td>\n",
       "      <td>518</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5886</td>\n",
       "      <td>1.7441</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>765.2800</td>\n",
       "      <td>488</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>26.3890</td>\n",
       "      <td>16.8276</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24341 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  recency  frequency    T  monetary_value  \\\n",
       "0               0        12346       90          6   74         28.0500   \n",
       "1               0        12349       32         46  146       1044.3700   \n",
       "2               0        12355       10         22    0        488.2100   \n",
       "3               0        12358      174         17    0       1429.8300   \n",
       "4               0        12359       49         66  128       1428.2300   \n",
       "...           ...          ...      ...        ...  ...             ...   \n",
       "24336           7        18273      126          1    0         51.0000   \n",
       "24337           7        18280      146         10    0        180.6000   \n",
       "24338           7        18281       49          7    0         80.8200   \n",
       "24339           7        18283       17        297  136        768.8200   \n",
       "24340           7        18287       70         29    0        765.2800   \n",
       "\n",
       "       total_quantity  purchase_hour_median  purchase_hour_dow  \\\n",
       "0                   6               13.0000             1.0000   \n",
       "1                 473               13.0000             3.0000   \n",
       "2                 303               11.0000             4.0000   \n",
       "3                 309                7.0000             1.0000   \n",
       "4                 777               13.0000             2.0000   \n",
       "...               ...                   ...                ...   \n",
       "24336              20               11.0000             6.0000   \n",
       "24337              45                9.0000             0.0000   \n",
       "24338              54               10.0000             6.0000   \n",
       "24339             518               13.0000             3.0000   \n",
       "24340             488               10.0000             6.0000   \n",
       "\n",
       "       purchase_weekend_prop  avg_basket_value  avg_basket_size  is_returned  \\\n",
       "0                     0.0000            4.6750           1.0000            1   \n",
       "1                     0.0000           22.7037          10.2826            0   \n",
       "2                     0.0000           22.1914          13.7727            0   \n",
       "3                     0.0000           84.1076          18.1765            1   \n",
       "4                     0.4571           21.6398          11.7727            1   \n",
       "...                      ...               ...              ...          ...   \n",
       "24336                 1.0000           51.0000          20.0000            1   \n",
       "24337                 0.0000           18.0600           4.5000            0   \n",
       "24338                 1.0000           11.5457           7.7143            0   \n",
       "24339                 0.0000            2.5886           1.7441            1   \n",
       "24340                 1.0000           26.3890          16.8276            0   \n",
       "\n",
       "       target_rev  \n",
       "0        142.3100  \n",
       "1          0.0000  \n",
       "2          0.0000  \n",
       "3        268.1000  \n",
       "4        489.8000  \n",
       "...           ...  \n",
       "24336    102.0000  \n",
       "24337      0.0000  \n",
       "24338      0.0000  \n",
       "24339    130.9000  \n",
       "24340      0.0000  \n",
       "\n",
       "[24341 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7875fddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rfm_period</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>3.5227</td>\n",
       "      <td>2.2075</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer ID</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>15324.1009</td>\n",
       "      <td>1700.2936</td>\n",
       "      <td>12346.0000</td>\n",
       "      <td>13861.0000</td>\n",
       "      <td>15303.0000</td>\n",
       "      <td>16797.0000</td>\n",
       "      <td>18287.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recency</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>63.6779</td>\n",
       "      <td>51.8604</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>103.0000</td>\n",
       "      <td>183.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>61.4619</td>\n",
       "      <td>114.9094</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>3304.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>52.3596</td>\n",
       "      <td>60.0054</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>106.0000</td>\n",
       "      <td>183.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monetary_value</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>1282.5121</td>\n",
       "      <td>4598.3053</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>252.8400</td>\n",
       "      <td>516.1300</td>\n",
       "      <td>1128.1500</td>\n",
       "      <td>164699.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_quantity</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>814.2763</td>\n",
       "      <td>3613.6183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>129.0000</td>\n",
       "      <td>292.0000</td>\n",
       "      <td>675.0000</td>\n",
       "      <td>220596.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_hour_median</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>12.6307</td>\n",
       "      <td>2.2192</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>20.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_hour_dow</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>2.5602</td>\n",
       "      <td>1.7783</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_weekend_prop</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.3010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_basket_value</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>32.9760</td>\n",
       "      <td>136.3594</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.9852</td>\n",
       "      <td>16.9911</td>\n",
       "      <td>24.5468</td>\n",
       "      <td>13206.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_basket_size</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>22.6927</td>\n",
       "      <td>118.4917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.3571</td>\n",
       "      <td>9.5200</td>\n",
       "      <td>14.5455</td>\n",
       "      <td>5118.1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_returned</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>0.3988</td>\n",
       "      <td>0.4897</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_rev</th>\n",
       "      <td>24341.0000</td>\n",
       "      <td>379.1863</td>\n",
       "      <td>1884.2018</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>329.4600</td>\n",
       "      <td>88660.5300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count       mean       std        min        25%  \\\n",
       "rfm_period            24341.0000     3.5227    2.2075     0.0000     2.0000   \n",
       "Customer ID           24341.0000 15324.1009 1700.2936 12346.0000 13861.0000   \n",
       "recency               24341.0000    63.6779   51.8604     0.0000    18.0000   \n",
       "frequency             24341.0000    61.4619  114.9094     0.0000    15.0000   \n",
       "T                     24341.0000    52.3596   60.0054     0.0000     0.0000   \n",
       "monetary_value        24341.0000  1282.5121 4598.3053     0.0000   252.8400   \n",
       "total_quantity        24341.0000   814.2763 3613.6183     0.0000   129.0000   \n",
       "purchase_hour_median  24341.0000    12.6307    2.2192     7.0000    11.0000   \n",
       "purchase_hour_dow     24341.0000     2.5602    1.7783     0.0000     1.0000   \n",
       "purchase_weekend_prop 24341.0000     0.1408    0.3010     0.0000     0.0000   \n",
       "avg_basket_value      24341.0000    32.9760  136.3594     0.0000    10.9852   \n",
       "avg_basket_size       24341.0000    22.6927  118.4917     0.0000     5.3571   \n",
       "is_returned           24341.0000     0.3988    0.4897     0.0000     0.0000   \n",
       "target_rev            24341.0000   379.1863 1884.2018     0.0000     0.0000   \n",
       "\n",
       "                             50%        75%         max  \n",
       "rfm_period                4.0000     5.0000      7.0000  \n",
       "Customer ID           15303.0000 16797.0000  18287.0000  \n",
       "recency                  52.0000   103.0000    183.0000  \n",
       "frequency                32.0000    69.0000   3304.0000  \n",
       "T                        21.0000   106.0000    183.0000  \n",
       "monetary_value          516.1300  1128.1500 164699.0700  \n",
       "total_quantity          292.0000   675.0000 220596.0000  \n",
       "purchase_hour_median     13.0000    14.0000     20.0000  \n",
       "purchase_hour_dow         2.0000     4.0000      6.0000  \n",
       "purchase_weekend_prop     0.0000     0.0000      1.0000  \n",
       "avg_basket_value         16.9911    24.5468  13206.5000  \n",
       "avg_basket_size           9.5200    14.5455   5118.1818  \n",
       "is_returned               0.0000     1.0000      1.0000  \n",
       "target_rev                0.0000   329.4600  88660.5300  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4132ac33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1600 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "whole_data.hist(figsize=(12,16))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a1d3d",
   "metadata": {},
   "source": [
    "It would be pretty hard if we only predict the revenues directly. We need to predict first if the customer will return. In this section, we'll be using two types of prediction: **(1) returning customers and (2) their target revenues**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92f585",
   "metadata": {},
   "source": [
    "## Prediction of Returning Customers and its Revenue "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a473af",
   "metadata": {},
   "source": [
    "For both predictions on machine learning, we'll first use `RandomizedSearchCV` on the first four periods (`0 <= rfm_period <= 3`) to find the best parameters or settings of the model. After that, we'll use it to predict both values on all periods, from `0-7`; `is_returned` and `log_target_rev`. The result will be:\n",
    "\n",
    "$$Predicted Revenue = P(classification) * value(regression)$$\n",
    "\n",
    "We will gather both predictions in a DataFrame, and then count the metrics. Here we're utilizing coefficient of determination (R^2) to see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f44ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of `X_test, y_test` start at `rfm_period == 0`\n",
    "predictions_list = pd.DataFrame(\n",
    "    whole_data.loc[whole_data['rfm_period'] >= 0, \n",
    "        ['rfm_period', 'Customer ID', 'is_returned']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42bfcdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12358</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24341 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned\n",
       "0               0        12346            1\n",
       "1               0        12349            0\n",
       "2               0        12355            0\n",
       "3               0        12358            1\n",
       "4               0        12359            1\n",
       "...           ...          ...          ...\n",
       "24336           7        18273            1\n",
       "24337           7        18280            0\n",
       "24338           7        18281            0\n",
       "24339           7        18283            1\n",
       "24340           7        18287            0\n",
       "\n",
       "[24341 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05ecbf",
   "metadata": {},
   "source": [
    "### Function 1: Grid Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb4ddd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFIED: Accuracy count separately; classification and then regression\n",
    "def grid_backtesting(df, grid_classifier, grid_regressor):\n",
    "    '''\n",
    "        Function to do cross-validation with given model of grid parameters\n",
    "        (`RandomizedSearchCV` or `GridSearchCV`) for `rfm_period` 0-3 (4 periods),\n",
    "        \n",
    "        then fitting the best parameters to dataset with `rfm_period` 4-8 (3 periods)\n",
    "        for both in classification and regression. For the combinations, we'll\n",
    "        multiply the predicted_revenue = Probability(Class) * Value(Reg), \n",
    "        afterwards we compare the predicted_revenue to real values, \n",
    "        find the R2 score on each iterations, and lastly average it out.      \n",
    "    '''\n",
    "    \n",
    "    grid_best_params_classifier = {}\n",
    "    grid_best_scores_classifier = {}\n",
    "    \n",
    "    grid_best_params_regressor = {}\n",
    "    grid_best_scores_regressor = {}\n",
    "    \n",
    "    acc_results = []\n",
    "    r2_results = []\n",
    "    hurdle_results = []\n",
    "    \n",
    "    ## Make a copy of IDs and labels so that we can add predictions result later on.\n",
    "    model_predictions = df.loc[df['rfm_period'] >= 1, \n",
    "        ['rfm_period', 'Customer ID', 'is_returned', 'target_rev']]\n",
    "    \n",
    "    print(f'Cross-validation for classification and regression:\\n')\n",
    "    print(f'{grid_classifier}\\n\\n{grid_regressor}\\n')\n",
    "    \n",
    "    ## Cross-validation: Period 0-3\n",
    "    for n_period in range(0, 4):\n",
    "        \n",
    "        mask_val = (df['rfm_period']  <= n_period)\n",
    "        mask_reg = (df['is_returned'] == 1)\n",
    "        \n",
    "        print(f'rfm_period {n_period+1}/4:')\n",
    "        \n",
    "        X_val_class, y_val_class = (\n",
    "            df.loc[mask_val, df.columns[2:-2]],\n",
    "            df.loc[mask_val, df.columns[-2]  ])\n",
    "\n",
    "        X_val_reg, y_val_reg = (\n",
    "            df.loc[mask_val & mask_reg, df.columns[2:-2]],\n",
    "            df.loc[mask_val & mask_reg, df.columns[-1]  ])\n",
    "        \n",
    "        ## Classification -- Predict customers that will return \n",
    "        grid_classifier.fit(X_val_class, y_val_class)\n",
    "        \n",
    "        grid_best_params_classifier[n_period+1] = grid_classifier.best_estimator_\n",
    "        grid_best_scores_classifier[n_period+1] = grid_classifier.best_score_\n",
    "        \n",
    "        ## Regression: Predict revenues of returned customer\n",
    "        grid_regressor.fit(X_val_reg, y_val_reg)\n",
    "        \n",
    "        grid_best_params_regressor[n_period+1] = grid_regressor.best_estimator_\n",
    "        grid_best_scores_regressor[n_period+1] = grid_regressor.best_score_\n",
    "    \n",
    "    print('\\nBest params for the model: \\n')\n",
    "    print(f'Classifier:\\n{grid_best_params_classifier[4]}\\nAccuracy: {grid_best_scores_classifier[4]}\\n')\n",
    "    print(f'Regressor:\\n{grid_best_params_regressor[4]}\\nAccuracy: {grid_best_scores_regressor[4]}\\n')\n",
    "    \n",
    "    print('Cross-validation done!\\n')\n",
    "                \n",
    "    print('Fitting best parameters into the next periods:')\n",
    "    used_model_classifier     = grid_best_params_classifier[4]\n",
    "    used_model_regressor      = grid_best_params_regressor[4]\n",
    "\n",
    "    \n",
    "    ## Fit the data into best parameters for both types of models:\n",
    "    for n_period in range(0, 7):\n",
    "\n",
    "        mask_train = (df['rfm_period'] <= n_period)\n",
    "        mask_test  = (df['rfm_period'] == n_period + 1)\n",
    "        mask_reg   = (df['is_returned'] == 1)\n",
    "        \n",
    "        print(f'rfm_period {n_period+1}/7:')\n",
    "        \n",
    "        X_train_class, y_train_class = (\n",
    "            df.loc[mask_train, df.columns[2:-2]],\n",
    "            df.loc[mask_train, df.columns[-2]  ])\n",
    "        X_test_class, y_test_class   = (\n",
    "            df.loc[mask_test, df.columns[2:-2]],\n",
    "            df.loc[mask_test, df.columns[-2]  ]) \n",
    "\n",
    "        X_train_reg, y_train_reg = (\n",
    "            df.loc[mask_train & mask_reg, df.columns[2:-2]],\n",
    "            df.loc[mask_train & mask_reg, df.columns[-1]  ])\n",
    "        X_test_reg, y_test_reg   = (\n",
    "            df.loc[mask_test  & mask_reg, df.columns[2:-2]],\n",
    "            df.loc[mask_test  & mask_reg, df.columns[-1]  ]) \n",
    "\n",
    "        ## Classification -- Predict customers that will return \n",
    "        used_model_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "        ## Regression: Predict revenues of returned customer\n",
    "        used_model_regressor.fit(X_train_reg, y_train_reg)\n",
    "        \n",
    "        # Some conditionals on different ensemble models (not all has `predict_proba`):\n",
    "\n",
    "## MODIFY TO GET THE PREDICT_PROBA AND APPLY HURDLE MODELS EVALUATION\n",
    "\n",
    "        # Put the classifier prediction into respective `rfm_period`\n",
    "        model_predictions.loc[mask_test, 'class_pred'] = (\n",
    "            used_model_classifier.predict(X_test_class))\n",
    "\n",
    "        # Put the classifier prediction into respective `rfm_period`\n",
    "        model_predictions.loc[mask_test, 'class_pred_proba'] = (\n",
    "            used_model_classifier.predict_proba(X_test_class)[:,1])\n",
    "\n",
    "        # Regression results: Return\n",
    "        model_predictions.loc[mask_test &  mask_reg, 'reg_pred'] = (\n",
    "            used_model_regressor.predict(X_test_reg))\n",
    "\n",
    "        # Treatment for others: Put it as zero\n",
    "        model_predictions.loc[mask_test & ~mask_reg, 'reg_pred'] = 0\n",
    "\n",
    "        # Calculate the predicted revenue\n",
    "        model_predictions.loc[mask_test, 'predicted_rev'] = (\n",
    "            model_predictions.loc[mask_test, 'class_pred_proba'] *\n",
    "            model_predictions.loc[mask_test, 'reg_pred']\n",
    "        )\n",
    "\n",
    "        acc_results.append(accuracy_score(\n",
    "            model_predictions.loc[mask_test, 'is_returned'],\n",
    "            model_predictions.loc[mask_test, 'class_pred']\n",
    "        ))\n",
    "\n",
    "        r2_results.append(r2_score(\n",
    "            model_predictions.loc[mask_test & mask_reg, 'target_rev'], \n",
    "            model_predictions.loc[mask_test & mask_reg, 'reg_pred']\n",
    "        ))\n",
    "\n",
    "        \n",
    "        hurdle_results.append(r2_score(\n",
    "            model_predictions.loc[mask_test & mask_reg, 'target_rev'], \n",
    "            model_predictions.loc[mask_test & mask_reg, 'predicted_rev']\n",
    "        ))\n",
    "          \n",
    "    print('Prediction done!')\n",
    "    \n",
    "    acc_avg    = np.mean(acc_results)\n",
    "    r2_avg     = np.mean(r2_results)\n",
    "    hurdle_avg = np.mean(hurdle_results)\n",
    "    \n",
    "    print(f'Classification accuracy: {acc_avg}')\n",
    "    print(f'Regression accuracy: {r2_avg}')\n",
    "    print(f'Class + Reg R2 Score -- Hurdle Models: {hurdle_avg}')\n",
    "\n",
    "    return acc_avg, r2_avg, hurdle_avg, model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4043e",
   "metadata": {},
   "source": [
    "### Function 2: Zero-Inflated Grid Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84721dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFIED: Accuracy count separately; classification and then regression\n",
    "def grid_zeroinflated_backtesting(df, grid_zir):\n",
    "    '''\n",
    "        Function to do cross-validation with given model of grid parameters\n",
    "        (`RandomizedSearchCV` or `GridSearchCV`) for `rfm_period` 0-3 (4 periods),\n",
    "        \n",
    "        then fitting the best parameters to dataset with `rfm_period` 4-8 (3 periods),\n",
    "        fit the data into model of ZeroInflatedRegressor, \n",
    "        afterwards we compare the predicted_revenue to real values, \n",
    "        find the R2 score on each iterations, and lastly average it out.      \n",
    "    '''\n",
    "    \n",
    "    grid_best_params_zir = {}\n",
    "    grid_best_scores_zir = {}\n",
    "    \n",
    "    hurdle_results       = []\n",
    "    \n",
    "    ## Make a copy of IDs and labels so that we can add predictions result later on.\n",
    "    model_predictions = df.loc[df['rfm_period'] >= 1, \n",
    "        ['rfm_period', 'Customer ID', 'is_returned', 'target_rev']]\n",
    "    \n",
    "    print(f'Cross-validation for ZIR (classification + regression):\\n')\n",
    "    print(f'{grid_zir.estimator}\\n')\n",
    "\n",
    "    ## Cross-validation: Period 0-3\n",
    "    for n_period in range(0, 4):\n",
    "        \n",
    "        mask_val = (df['rfm_period']  <= n_period)\n",
    "        \n",
    "        print(f'rfm_period {n_period+1}/4:')\n",
    "        \n",
    "        X_val, y_val = (\n",
    "            df.loc[mask_val, df.columns[2:-2]],\n",
    "            df.loc[mask_val, df.columns[-1]  ])\n",
    "        \n",
    "        ## Classification -- Predict customers that will return \n",
    "        grid_zir.fit(X_val, y_val)\n",
    "        \n",
    "        grid_best_params_zir[n_period+1] = grid_zir.best_estimator_\n",
    "        grid_best_scores_zir[n_period+1] = grid_zir.best_score_\n",
    "            \n",
    "    print('\\nBest params for the model: \\n')\n",
    "    print(f'ZIR (Classifier+Regressor) :\\n{grid_best_params_zir[4]}\\nAccuracy: {grid_best_scores_zir[4]}\\n')\n",
    "    \n",
    "    print('Cross-validation done!\\n')\n",
    "        \n",
    "    print('Fitting best parameters into the next periods:')\n",
    "    zir_best_params     = grid_best_params_zir[4]\n",
    "    \n",
    "    ## Fit the data into best parameters for both types of models:\n",
    "    for n_period in range(0, 7):\n",
    "        \n",
    "        print(f'rfm_period {n_period+1}/7:')\n",
    "        \n",
    "        mask_train = (df['rfm_period'] <= n_period)\n",
    "        mask_test  = (df['rfm_period'] == n_period + 1)\n",
    "        \n",
    "        X_train, y_train = (\n",
    "            df.loc[mask_train, df.columns[2:-2]],\n",
    "            df.loc[mask_train, df.columns[-1]])\n",
    "        X_test, y_test   = (\n",
    "            df.loc[mask_test , df.columns[2:-2]],\n",
    "            df.loc[mask_test , df.columns[-1]]) \n",
    "\n",
    "        ## Class + Reg -- Predict customers that will return with its revenue\n",
    "        zir_best_params.fit(X_train, y_train)\n",
    "\n",
    "        # Regression results: Return\n",
    "        model_predictions.loc[mask_test, 'predicted_rev'] = (\n",
    "            zir_best_params.predict(X_test))\n",
    "\n",
    "        hurdle_results.append(r2_score(\n",
    "            model_predictions.loc[mask_test, 'target_rev'], \n",
    "            model_predictions.loc[mask_test, 'predicted_rev']\n",
    "        ))\n",
    "          \n",
    "    print('Prediction done!')\n",
    "    \n",
    "    hurdle_avg = np.mean(hurdle_results)\n",
    "    print(f'Class + Reg R2 Score -- Hurdle Models: {hurdle_avg}')\n",
    "\n",
    "    return hurdle_avg, model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8f87a",
   "metadata": {},
   "source": [
    "In summary, both functions will work as follows:\n",
    "\n",
    "* All models will go through validation via `RandomizedSearchCV`. Considering the time-quality tradeoff, 250 candidates will run through a 3-fold cross-validation with dataset from `rfm_period` of `0` to `3`. \n",
    "\n",
    "* The validation is conducted on two ways:\n",
    "    * Classification and regression separated (2 accuracies) conducted in function of `grid_backtesting`, and \n",
    "    * Classification and regression combined (1 accuracies) using `ZeroInflatedRegressor` in module `scikit-lego` conducted in function of `grid_zeroinflated_backtesting`.\n",
    "\n",
    "* The best hyperparameters will be used as the model for the prediction and will run through training-test on a whole dataset, from `rfm_period` of `0` to `7`. \n",
    "* Out of the three tested models, best model will run through `GridSearchCV` with all parameters considered to see if there's a better parameter resulting in best accuracy for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5d072",
   "metadata": {},
   "source": [
    "### Model 1: `RandomForest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cb1a3",
   "metadata": {},
   "source": [
    "#### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbeae7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_params_rf_class = {\n",
    "         'n_estimators': [100, 150, 200, 250, 300],\n",
    "            'max_depth': [2, 5, 7, 9, 10],\n",
    "    'min_samples_split': [2, 3, 5, 7],\n",
    "     'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_class = RandomForestClassifier()\n",
    "\n",
    "random_search_rf_class = RandomizedSearchCV(rf_class,\n",
    "    random_params_rf_class, n_iter=250, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "random_params_rf_reg = {\n",
    "         'n_estimators': [100, 150, 200, 250, 300],\n",
    "            'max_depth': [2, 5, 7, 9, 10],\n",
    "    'min_samples_split': [2, 3, 5, 7],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "         'max_features': [1, 6, 'sqrt'],\n",
    "            'bootstrap': [False, True]\n",
    "}\n",
    "\n",
    "rf_reg = RandomForestRegressor()\n",
    "\n",
    "random_search_rf_reg = RandomizedSearchCV(rf_reg,\n",
    "    random_params_rf_reg, n_iter=250, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f2aef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_rf_class.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc6dbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_rf_reg.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef92ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for classification and regression:\n",
      "\n",
      "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=250,\n",
      "                   n_jobs=-1,\n",
      "                   param_distributions={'max_depth': [2, 5, 7, 9, 10],\n",
      "                                        'min_samples_leaf': [1, 2, 4],\n",
      "                                        'min_samples_split': [2, 3, 5, 7],\n",
      "                                        'n_estimators': [100, 150, 200, 250,\n",
      "                                                         300]},\n",
      "                   verbose=1)\n",
      "\n",
      "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=250,\n",
      "                   n_jobs=-1,\n",
      "                   param_distributions={'bootstrap': [False, True],\n",
      "                                        'max_depth': [2, 5, 7, 9, 10],\n",
      "                                        'max_features': [1, 'sqrt'],\n",
      "                                        'min_samples_leaf': [1, 2, 4],\n",
      "                                        'min_samples_split': [2, 3, 5, 7],\n",
      "                                        'n_estimators': [100, 150, 200, 250,\n",
      "                                                         300]},\n",
      "                   verbose=1)\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "Classifier:\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=2, min_samples_split=7,\n",
      "                       n_estimators=250)\n",
      "Accuracy: 0.6835577659413156\n",
      "\n",
      "Regressor:\n",
      "RandomForestRegressor(max_depth=9, max_features='sqrt', min_samples_leaf=2,\n",
      "                      min_samples_split=5)\n",
      "Accuracy: 0.6578797074055159\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "rfm_period 2/7:\n",
      "rfm_period 3/7:\n",
      "rfm_period 4/7:\n",
      "rfm_period 5/7:\n",
      "rfm_period 6/7:\n",
      "rfm_period 7/7:\n",
      "Prediction done!\n",
      "Classification accuracy: 0.6926833642150635\n",
      "Regression accuracy: 0.5723393188749951\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.5737303247965023\n"
     ]
    }
   ],
   "source": [
    "acc_avg_rf, r2_avg_rf, hurdle_avg_rf, rf_predictions = grid_backtesting(\n",
    "    whole_data, random_search_rf_class, random_search_rf_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e5fc295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>class_pred</th>\n",
       "      <th>class_pred_proba</th>\n",
       "      <th>reg_pred</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4435</td>\n",
       "      <td>436.8752</td>\n",
       "      <td>193.7595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>534.0076</td>\n",
       "      <td>150.8613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5947</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>467.0054</td>\n",
       "      <td>115.5575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5221</td>\n",
       "      <td>510.0976</td>\n",
       "      <td>266.3230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6284</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1931</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>437.9276</td>\n",
       "      <td>141.1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1949</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6298</td>\n",
       "      <td>678.6142</td>\n",
       "      <td>427.3719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1846</td>\n",
       "      <td>318.2815</td>\n",
       "      <td>58.7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1908</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5623</td>\n",
       "      <td>525.6110</td>\n",
       "      <td>295.5742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  class_pred  \\\n",
       "24321           7        18241            1    976.0900      0.0000   \n",
       "24322           7        18242            1   1538.4100      0.0000   \n",
       "24323           7        18245            0      0.0000      1.0000   \n",
       "24324           7        18246            0      0.0000      0.0000   \n",
       "24325           7        18248            1    286.5600      0.0000   \n",
       "24326           7        18250            0      0.0000      0.0000   \n",
       "24327           7        18252            0      0.0000      0.0000   \n",
       "24328           7        18257            1    627.2700      1.0000   \n",
       "24329           7        18260            0      0.0000      1.0000   \n",
       "24330           7        18262            0      0.0000      0.0000   \n",
       "24331           7        18263            0      0.0000      1.0000   \n",
       "24332           7        18265            1    312.9600      0.0000   \n",
       "24333           7        18268            0      0.0000      0.0000   \n",
       "24334           7        18270            0      0.0000      0.0000   \n",
       "24335           7        18272            1    372.2500      1.0000   \n",
       "24336           7        18273            1    102.0000      0.0000   \n",
       "24337           7        18280            0      0.0000      0.0000   \n",
       "24338           7        18281            0      0.0000      0.0000   \n",
       "24339           7        18283            1    130.9000      1.0000   \n",
       "24340           7        18287            0      0.0000      0.0000   \n",
       "\n",
       "       class_pred_proba  reg_pred  predicted_rev  \n",
       "24321            0.4435  436.8752       193.7595  \n",
       "24322            0.2825  534.0076       150.8613  \n",
       "24323            0.5947    0.0000         0.0000  \n",
       "24324            0.1891    0.0000         0.0000  \n",
       "24325            0.2474  467.0054       115.5575  \n",
       "24326            0.2251    0.0000         0.0000  \n",
       "24327            0.1769    0.0000         0.0000  \n",
       "24328            0.5221  510.0976       266.3230  \n",
       "24329            0.6284    0.0000         0.0000  \n",
       "24330            0.1931    0.0000         0.0000  \n",
       "24331            0.5172    0.0000         0.0000  \n",
       "24332            0.3222  437.9276       141.1103  \n",
       "24333            0.2730    0.0000         0.0000  \n",
       "24334            0.1949    0.0000         0.0000  \n",
       "24335            0.6298  678.6142       427.3719  \n",
       "24336            0.1846  318.2815        58.7531  \n",
       "24337            0.1908    0.0000         0.0000  \n",
       "24338            0.2057    0.0000         0.0000  \n",
       "24339            0.5623  525.6110       295.5742  \n",
       "24340            0.2988    0.0000         0.0000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fc8a1",
   "metadata": {},
   "source": [
    "#### Zero-Inflated Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f2b465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_params_rf_zir = {\n",
    "         'classifier__n_estimators': [100, 150, 200, 250, 300],\n",
    "            'classifier__max_depth': [2, 5, 7, 9, 10],\n",
    "    'classifier__min_samples_split': [2, 3, 5, 7],\n",
    "     'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    \n",
    "          'regressor__n_estimators': [100, 150, 200, 250, 300],\n",
    "             'regressor__max_depth': [2, 5, 7, 9, 10],\n",
    "     'regressor__min_samples_split': [2, 3, 5, 7],\n",
    "      'regressor__min_samples_leaf': [1, 2, 4],\n",
    "          'regressor__max_features': [1, 6, 'sqrt'],\n",
    "             'regressor__bootstrap': [False, True]\n",
    "}\n",
    "\n",
    "rf_zir = ZeroInflatedRegressor(\n",
    "    classifier=RandomForestClassifier(),\n",
    "    regressor =RandomForestRegressor()\n",
    ")\n",
    "\n",
    "random_search_rf_zir = RandomizedSearchCV(\n",
    "    rf_zir, random_params_rf_zir, n_iter=250, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f3a4ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
       "                      regressor=RandomForestRegressor())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ZeroInflatedRegressor</label><div class=\"sk-toggleable__content\"><pre>ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
       "                      regressor=RandomForestRegressor())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">regressor: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
       "                      regressor=RandomForestRegressor())"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_rf_zir.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50d7a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for ZIR (classification + regression):\n",
      "\n",
      "ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
      "                      regressor=RandomForestRegressor())\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "ZIR (Classifier+Regressor) :\n",
      "ZeroInflatedRegressor(classifier=RandomForestClassifier(max_depth=9,\n",
      "                                                        min_samples_leaf=4,\n",
      "                                                        min_samples_split=7,\n",
      "                                                        n_estimators=250),\n",
      "                      regressor=RandomForestRegressor(max_depth=5,\n",
      "                                                      max_features='sqrt',\n",
      "                                                      min_samples_split=3,\n",
      "                                                      n_estimators=250))\n",
      "Accuracy: 0.6251959361544048\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "rfm_period 2/7:\n",
      "rfm_period 3/7:\n",
      "rfm_period 4/7:\n",
      "rfm_period 5/7:\n",
      "rfm_period 6/7:\n",
      "rfm_period 7/7:\n",
      "Prediction done!\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.5553419546639864\n"
     ]
    }
   ],
   "source": [
    "hurdle_avg_rf_zir, rf_zir_predictions = (\n",
    "    grid_zeroinflated_backtesting(whole_data, random_search_rf_zir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9e5dd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>592.9041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>500.0843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>657.1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>634.9441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>677.0876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>540.0340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  predicted_rev\n",
       "24321           7        18241            1    976.0900         0.0000\n",
       "24322           7        18242            1   1538.4100         0.0000\n",
       "24323           7        18245            0      0.0000       592.9041\n",
       "24324           7        18246            0      0.0000         0.0000\n",
       "24325           7        18248            1    286.5600         0.0000\n",
       "24326           7        18250            0      0.0000         0.0000\n",
       "24327           7        18252            0      0.0000         0.0000\n",
       "24328           7        18257            1    627.2700       500.0843\n",
       "24329           7        18260            0      0.0000       657.1035\n",
       "24330           7        18262            0      0.0000         0.0000\n",
       "24331           7        18263            0      0.0000       634.9441\n",
       "24332           7        18265            1    312.9600         0.0000\n",
       "24333           7        18268            0      0.0000         0.0000\n",
       "24334           7        18270            0      0.0000         0.0000\n",
       "24335           7        18272            1    372.2500       677.0876\n",
       "24336           7        18273            1    102.0000         0.0000\n",
       "24337           7        18280            0      0.0000         0.0000\n",
       "24338           7        18281            0      0.0000         0.0000\n",
       "24339           7        18283            1    130.9000       540.0340\n",
       "24340           7        18287            0      0.0000         0.0000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_zir_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594016f",
   "metadata": {},
   "source": [
    "### Model 2: Light Gradient-Boosting Machine (`LGBM`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f1cce",
   "metadata": {},
   "source": [
    "#### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd5ffdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_params_lgbm_class = {\n",
    "        'learning_rate': [0.1, 0.01, 0.005],\n",
    "         'n_estimators': [40, 100, 200],\n",
    "            'max_depth': [-1, 3, 6],\n",
    "           'num_leaves': [31, 10, 15],\n",
    "        'boosting_type': ['gbdt'],\n",
    "            'objective': ['binary'],\n",
    "               'metric': ['binary_logloss'],\n",
    "     'colsample_bytree': [0.6, 0.8, 1],\n",
    "            'subsample': [0.7, 0.9, 1],\n",
    "            'reg_alpha': [0, 0.1, 0.01],\n",
    "           'reg_lambda': [0, 0.1, 0.01],\n",
    "    'min_child_samples': [10, 20],\n",
    "#       'is_unbalanced': [True],\n",
    "     'scale_pos_weight': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "lgbm_class = LGBMClassifier()\n",
    "\n",
    "random_search_lgbm_class = RandomizedSearchCV(lgbm_class,\n",
    "    random_params_lgbm_class, n_iter=250, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "random_params_lgbm_reg = {\n",
    "        'learning_rate': [0.1, 0.01, 0.005],\n",
    "         'n_estimators': [40, 100, 200],\n",
    "            'max_depth': [-1, 3, 6],\n",
    "           'num_leaves': [31, 10, 15],\n",
    "        'boosting_type': ['gbdt'],\n",
    "            'objective': ['regression'],\n",
    "               'metric': ['rmse'],\n",
    "     'colsample_bytree': [0.75, 0.8, 1],\n",
    "            'subsample': [0.75, 0.8, 1],\n",
    "            'reg_alpha': [0, 0.1, 0.01],\n",
    "           'reg_lambda': [0, 0.1, 0.01],\n",
    "    'min_child_samples': [10, 20],\n",
    "#        'is_unbalanced': [True],\n",
    "     'scale_pos_weight': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "lgbm_reg = LGBMRegressor()\n",
    "\n",
    "random_search_lgbm_reg = RandomizedSearchCV(lgbm_reg,\n",
    "    random_params_lgbm_reg, n_iter=250, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a347224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_lgbm_class.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6aef6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_lgbm_reg.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dce72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for classification and regression:\n",
      "\n",
      "RandomizedSearchCV(cv=3, estimator=LGBMClassifier(), n_iter=250, n_jobs=-1,\n",
      "                   param_distributions={'boosting_type': ['gbdt'],\n",
      "                                        'colsample_bytree': [0.6, 0.8, 1],\n",
      "                                        'learning_rate': [0.1, 0.01, 0.005],\n",
      "                                        'max_depth': [-1, 3, 6],\n",
      "                                        'metric': ['binary_logloss'],\n",
      "                                        'min_child_samples': [10, 20],\n",
      "                                        'n_estimators': [40, 100, 200],\n",
      "                                        'num_leaves': [31, 10, 15],\n",
      "                                        'objective': ['binary'],\n",
      "                                        'reg_alpha': [0, 0.1, 0.01],\n",
      "                                        'reg_lambda': [0, 0.1, 0.01],\n",
      "                                        'scale_pos_weight': [1.0, 1.1, 1.2],\n",
      "                                        'subsample': [0.7, 0.9, 1]},\n",
      "                   verbose=1)\n",
      "\n",
      "RandomizedSearchCV(cv=3, estimator=LGBMRegressor(), n_iter=250, n_jobs=-1,\n",
      "                   param_distributions={'boosting_type': ['gbdt'],\n",
      "                                        'colsample_bytree': [0.75, 0.8, 1],\n",
      "                                        'learning_rate': [0.1, 0.01, 0.005],\n",
      "                                        'max_depth': [-1, 3, 6],\n",
      "                                        'metric': ['rmse'],\n",
      "                                        'min_child_samples': [10, 20],\n",
      "                                        'n_estimators': [40, 100, 200],\n",
      "                                        'num_leaves': [31, 10, 15],\n",
      "                                        'objective': ['regression'],\n",
      "                                        'reg_alpha': [0, 0.1, 0.01],\n",
      "                                        'reg_lambda': [0, 0.1, 0.01],\n",
      "                                        'scale_pos_weight': [1.0, 1.1, 1.2],\n",
      "                                        'subsample': [0.75, 0.8, 1]},\n",
      "                   verbose=1)\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 1124, number of negative: 1604\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1858\n",
      "[LightGBM] [Info] Number of data points in the train set: 2728, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.412023 -> initscore=-0.355607\n",
      "[LightGBM] [Info] Start training from score -0.355607\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1672\n",
      "[LightGBM] [Info] Number of data points in the train set: 1124, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 890.433908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2369, number of negative: 3209\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1922\n",
      "[LightGBM] [Info] Number of data points in the train set: 5578, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.424704 -> initscore=-0.303491\n",
      "[LightGBM] [Info] Start training from score -0.303491\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1895\n",
      "[LightGBM] [Info] Number of data points in the train set: 2369, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 906.210093\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 3952, number of negative: 4577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 8529, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463360 -> initscore=-0.146822\n",
      "[LightGBM] [Info] Start training from score -0.146822\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1926\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 986.020907\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5061, number of negative: 6976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 12037, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.420454 -> initscore=-0.320912\n",
      "[LightGBM] [Info] Start training from score -0.320912\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5061, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1014.077258\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "Classifier:\n",
      "LGBMClassifier(colsample_bytree=0.6, learning_rate=0.01,\n",
      "               metric='binary_logloss', min_child_samples=10, n_estimators=200,\n",
      "               num_leaves=15, objective='binary', reg_alpha=0.1, reg_lambda=0.1,\n",
      "               scale_pos_weight=1.1, subsample=1)\n",
      "Accuracy: 0.6829760531512862\n",
      "\n",
      "Regressor:\n",
      "LGBMRegressor(colsample_bytree=0.8, max_depth=6, metric='rmse',\n",
      "              min_child_samples=10, num_leaves=10, objective='regression',\n",
      "              reg_alpha=0, reg_lambda=0.01, scale_pos_weight=1.2,\n",
      "              subsample=0.75)\n",
      "Accuracy: 0.6144044440313053\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "[LightGBM] [Info] Number of positive: 1124, number of negative: 1604\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1858\n",
      "[LightGBM] [Info] Number of data points in the train set: 2728, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.412023 -> initscore=-0.355607\n",
      "[LightGBM] [Info] Start training from score -0.355607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1672\n",
      "[LightGBM] [Info] Number of data points in the train set: 1124, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 890.433908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 2/7:\n",
      "[LightGBM] [Info] Number of positive: 2369, number of negative: 3209\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1922\n",
      "[LightGBM] [Info] Number of data points in the train set: 5578, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.424704 -> initscore=-0.303491\n",
      "[LightGBM] [Info] Start training from score -0.303491\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1895\n",
      "[LightGBM] [Info] Number of data points in the train set: 2369, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 906.210093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 3/7:\n",
      "[LightGBM] [Info] Number of positive: 3952, number of negative: 4577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 8529, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463360 -> initscore=-0.146822\n",
      "[LightGBM] [Info] Start training from score -0.146822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1926\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 986.020907\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 4/7:\n",
      "[LightGBM] [Info] Number of positive: 5061, number of negative: 6976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 12037, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.420454 -> initscore=-0.320912\n",
      "[LightGBM] [Info] Start training from score -0.320912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5061, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1014.077258\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 5/7:\n",
      "[LightGBM] [Info] Number of positive: 6128, number of negative: 9308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1934\n",
      "[LightGBM] [Info] Number of data points in the train set: 15436, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.396994 -> initscore=-0.418006\n",
      "[LightGBM] [Info] Start training from score -0.418006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1930\n",
      "[LightGBM] [Info] Number of data points in the train set: 6128, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 978.959740\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 6/7:\n",
      "[LightGBM] [Info] Number of positive: 7311, number of negative: 11476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1934\n",
      "[LightGBM] [Info] Number of data points in the train set: 18787, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389152 -> initscore=-0.450878\n",
      "[LightGBM] [Info] Start training from score -0.450878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 7311, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 950.524688\n",
      "rfm_period 7/7:\n",
      "[LightGBM] [Info] Number of positive: 8465, number of negative: 13080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1934\n",
      "[LightGBM] [Info] Number of data points in the train set: 21545, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.392899 -> initscore=-0.435144\n",
      "[LightGBM] [Info] Start training from score -0.435144\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 8465, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 941.538830\n",
      "Prediction done!\n",
      "Classification accuracy: 0.6908953109897328\n",
      "Regression accuracy: 0.5088887493591046\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.536379991193118\n"
     ]
    }
   ],
   "source": [
    "r2_avg_lgbm, r2_avg_lgbm, hurdle_avg_lgbm, lgbm_predictions = grid_backtesting(\n",
    "    whole_data, random_search_lgbm_class, random_search_lgbm_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc4f6c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>class_pred</th>\n",
       "      <th>class_pred_proba</th>\n",
       "      <th>reg_pred</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>415.4546</td>\n",
       "      <td>186.1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>572.2410</td>\n",
       "      <td>171.1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5990</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>477.0815</td>\n",
       "      <td>137.4649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2653</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5424</td>\n",
       "      <td>465.2460</td>\n",
       "      <td>252.3722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5959</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5277</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3366</td>\n",
       "      <td>466.0579</td>\n",
       "      <td>156.8878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>639.2660</td>\n",
       "      <td>392.0391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>362.9366</td>\n",
       "      <td>83.4701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2313</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>470.2592</td>\n",
       "      <td>283.5136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  class_pred  \\\n",
       "24321           7        18241            1    976.0900      0.0000   \n",
       "24322           7        18242            1   1538.4100      0.0000   \n",
       "24323           7        18245            0      0.0000      1.0000   \n",
       "24324           7        18246            0      0.0000      0.0000   \n",
       "24325           7        18248            1    286.5600      0.0000   \n",
       "24326           7        18250            0      0.0000      0.0000   \n",
       "24327           7        18252            0      0.0000      0.0000   \n",
       "24328           7        18257            1    627.2700      1.0000   \n",
       "24329           7        18260            0      0.0000      1.0000   \n",
       "24330           7        18262            0      0.0000      0.0000   \n",
       "24331           7        18263            0      0.0000      1.0000   \n",
       "24332           7        18265            1    312.9600      0.0000   \n",
       "24333           7        18268            0      0.0000      0.0000   \n",
       "24334           7        18270            0      0.0000      0.0000   \n",
       "24335           7        18272            1    372.2500      1.0000   \n",
       "24336           7        18273            1    102.0000      0.0000   \n",
       "24337           7        18280            0      0.0000      0.0000   \n",
       "24338           7        18281            0      0.0000      0.0000   \n",
       "24339           7        18283            1    130.9000      1.0000   \n",
       "24340           7        18287            0      0.0000      0.0000   \n",
       "\n",
       "       class_pred_proba  reg_pred  predicted_rev  \n",
       "24321            0.4480  415.4546       186.1414  \n",
       "24322            0.2990  572.2410       171.1174  \n",
       "24323            0.5990    0.0000         0.0000  \n",
       "24324            0.2370    0.0000         0.0000  \n",
       "24325            0.2881  477.0815       137.4649  \n",
       "24326            0.2653    0.0000         0.0000  \n",
       "24327            0.2429    0.0000         0.0000  \n",
       "24328            0.5424  465.2460       252.3722  \n",
       "24329            0.5959    0.0000         0.0000  \n",
       "24330            0.2458    0.0000         0.0000  \n",
       "24331            0.5277    0.0000         0.0000  \n",
       "24332            0.3366  466.0579       156.8878  \n",
       "24333            0.2337    0.0000         0.0000  \n",
       "24334            0.2379    0.0000         0.0000  \n",
       "24335            0.6133  639.2660       392.0391  \n",
       "24336            0.2300  362.9366        83.4701  \n",
       "24337            0.2313    0.0000         0.0000  \n",
       "24338            0.2473    0.0000         0.0000  \n",
       "24339            0.6029  470.2592       283.5136  \n",
       "24340            0.3386    0.0000         0.0000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea03296",
   "metadata": {},
   "source": [
    "#### Zero-Inflated Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f29f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options in grid_params for ZIR are\n",
    "## decreased for time efficiency\n",
    "\n",
    "### Total params combinations: 2592\n",
    "random_params_lgbm_zir = {\n",
    "        'classifier__learning_rate': [0.1, 0.01, 0.005],\n",
    "         'classifier__n_estimators': [40, 100, 200],\n",
    "            'classifier__max_depth': [-1, 3, 6],\n",
    "           'classifier__num_leaves': [31, 10, 15],\n",
    "        'classifier__boosting_type': ['gbdt'],\n",
    "            'classifier__objective': ['binary'],\n",
    "               'classifier__metric': ['binary_logloss'],\n",
    "     'classifier__colsample_bytree': [0.75, 0.8, 1],\n",
    "            'classifier__subsample': [0.75, 0.8, 1],\n",
    "            'classifier__reg_alpha': [0, 0.1, 0.01],\n",
    "           'classifier__reg_lambda': [0, 0.1, 0.01],\n",
    "    'classifier__min_child_samples': [10, 20],\n",
    "#       'classifier__is_unbalanced': [True],\n",
    "     'classifier__scale_pos_weight': [1.0, 1.1, 1.2],\n",
    "    \n",
    "         'regressor__learning_rate': [0.1, 0.01, 0.005],\n",
    "          'regressor__n_estimators': [40, 100, 200],\n",
    "             'regressor__max_depth': [-1, 3, 6],\n",
    "            'regressor__num_leaves': [31, 10, 15],\n",
    "         'regressor__boosting_type': ['gbdt'],\n",
    "             'regressor__objective': ['regression'],\n",
    "                'regressor__metric': ['rmse'],\n",
    "      'regressor__colsample_bytree': [0.75, 0.8, 1],\n",
    "             'regressor__subsample': [0.75, 0.8, 1],\n",
    "             'regressor__reg_alpha': [0, 0.1, 0.01],\n",
    "            'regressor__reg_lambda': [0, 0.1, 0.01],\n",
    "     'regressor__min_child_samples': [10, 20],\n",
    "#        'regressor__is_unbalanced': [True],\n",
    "      'regressor__scale_pos_weight': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "lgbm_zir = ZeroInflatedRegressor(\n",
    "    classifier=LGBMClassifier(),\n",
    "    regressor =LGBMRegressor()\n",
    ")\n",
    "\n",
    "random_search_lgbm_zir = RandomizedSearchCV(lgbm_zir, \n",
    "    random_params_lgbm_zir, n_iter=250, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0c32fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ZeroInflatedRegressor(classifier=LGBMClassifier(), regressor=LGBMRegressor())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ZeroInflatedRegressor</label><div class=\"sk-toggleable__content\"><pre>ZeroInflatedRegressor(classifier=LGBMClassifier(), regressor=LGBMRegressor())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">regressor: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ZeroInflatedRegressor(classifier=LGBMClassifier(), regressor=LGBMRegressor())"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_lgbm_zir.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9b528a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for ZIR (classification + regression):\n",
      "\n",
      "ZeroInflatedRegressor(classifier=LGBMClassifier(), regressor=LGBMRegressor())\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 1123, number of negative: 1605\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1858\n",
      "[LightGBM] [Info] Number of data points in the train set: 2728, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.411657 -> initscore=-0.357120\n",
      "[LightGBM] [Info] Start training from score -0.357120\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1672\n",
      "[LightGBM] [Info] Number of data points in the train set: 1123, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 891.226814\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 2367, number of negative: 3211\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1922\n",
      "[LightGBM] [Info] Number of data points in the train set: 5578, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.424346 -> initscore=-0.304959\n",
      "[LightGBM] [Info] Start training from score -0.304959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1895\n",
      "[LightGBM] [Info] Number of data points in the train set: 2367, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 906.975796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 3949, number of negative: 4580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 8529, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463009 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1926\n",
      "[LightGBM] [Info] Number of data points in the train set: 3949, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 986.769974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 5058, number of negative: 6979\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 12037, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.420204 -> initscore=-0.321934\n",
      "[LightGBM] [Info] Start training from score -0.321934\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5058, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1014.678728\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "ZIR (Classifier+Regressor) :\n",
      "ZeroInflatedRegressor(classifier=LGBMClassifier(colsample_bytree=0.8,\n",
      "                                                learning_rate=0.005,\n",
      "                                                max_depth=3,\n",
      "                                                metric='binary_logloss',\n",
      "                                                num_leaves=15,\n",
      "                                                objective='binary', reg_alpha=0,\n",
      "                                                reg_lambda=0,\n",
      "                                                scale_pos_weight=1.1,\n",
      "                                                subsample=1),\n",
      "                      regressor=LGBMRegressor(colsample_bytree=1, max_depth=3,\n",
      "                                              metric='rmse',\n",
      "                                              min_child_samples=10,\n",
      "                                              n_estimators=40, num_leaves=10,\n",
      "                                              objective='regression',\n",
      "                                              reg_alpha=0.1, reg_lambda=0.1,\n",
      "                                              scale_pos_weight=1.0,\n",
      "                                              subsample=0.8))\n",
      "Accuracy: 0.561512997349531\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "[LightGBM] [Info] Number of positive: 1123, number of negative: 1605\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1858\n",
      "[LightGBM] [Info] Number of data points in the train set: 2728, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.411657 -> initscore=-0.357120\n",
      "[LightGBM] [Info] Start training from score -0.357120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1672\n",
      "[LightGBM] [Info] Number of data points in the train set: 1123, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 891.226814\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfm_period 2/7:\n",
      "[LightGBM] [Info] Number of positive: 2367, number of negative: 3211\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1922\n",
      "[LightGBM] [Info] Number of data points in the train set: 5578, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.424346 -> initscore=-0.304959\n",
      "[LightGBM] [Info] Start training from score -0.304959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1895\n",
      "[LightGBM] [Info] Number of data points in the train set: 2367, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 906.975796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 3/7:\n",
      "[LightGBM] [Info] Number of positive: 3949, number of negative: 4580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 8529, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463009 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1926\n",
      "[LightGBM] [Info] Number of data points in the train set: 3949, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 986.769974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 4/7:\n",
      "[LightGBM] [Info] Number of positive: 5058, number of negative: 6979\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 12037, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.420204 -> initscore=-0.321934\n",
      "[LightGBM] [Info] Start training from score -0.321934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5058, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1014.678728\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 5/7:\n",
      "[LightGBM] [Info] Number of positive: 6124, number of negative: 9312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1934\n",
      "[LightGBM] [Info] Number of data points in the train set: 15436, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.396735 -> initscore=-0.419088\n",
      "[LightGBM] [Info] Start training from score -0.419088\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1930\n",
      "[LightGBM] [Info] Number of data points in the train set: 6124, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 979.599165\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 6/7:\n",
      "[LightGBM] [Info] Number of positive: 7305, number of negative: 11482\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1934\n",
      "[LightGBM] [Info] Number of data points in the train set: 18787, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.388833 -> initscore=-0.452222\n",
      "[LightGBM] [Info] Start training from score -0.452222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 7305, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 951.305407\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "rfm_period 7/7:\n",
      "[LightGBM] [Info] Number of positive: 8459, number of negative: 13086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1934\n",
      "[LightGBM] [Info] Number of data points in the train set: 21545, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.392620 -> initscore=-0.436312\n",
      "[LightGBM] [Info] Start training from score -0.436312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 8459, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 942.206667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Prediction done!\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.4826229539245059\n"
     ]
    }
   ],
   "source": [
    "hurdle_avg_lgbm_zir, lgbm_zir_predictions = (\n",
    "    grid_zeroinflated_backtesting(whole_data, random_search_lgbm_zir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bafb091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  predicted_rev\n",
       "24321           7        18241            1    976.0900         0.0000\n",
       "24322           7        18242            1   1538.4100         0.0000\n",
       "24323           7        18245            0      0.0000         0.0000\n",
       "24324           7        18246            0      0.0000         0.0000\n",
       "24325           7        18248            1    286.5600         0.0000\n",
       "24326           7        18250            0      0.0000         0.0000\n",
       "24327           7        18252            0      0.0000         0.0000\n",
       "24328           7        18257            1    627.2700         0.0000\n",
       "24329           7        18260            0      0.0000         0.0000\n",
       "24330           7        18262            0      0.0000         0.0000\n",
       "24331           7        18263            0      0.0000         0.0000\n",
       "24332           7        18265            1    312.9600         0.0000\n",
       "24333           7        18268            0      0.0000         0.0000\n",
       "24334           7        18270            0      0.0000         0.0000\n",
       "24335           7        18272            1    372.2500         0.0000\n",
       "24336           7        18273            1    102.0000         0.0000\n",
       "24337           7        18280            0      0.0000         0.0000\n",
       "24338           7        18281            0      0.0000         0.0000\n",
       "24339           7        18283            1    130.9000         0.0000\n",
       "24340           7        18287            0      0.0000         0.0000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_zir_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b6323",
   "metadata": {},
   "source": [
    "### Model 3: eXtreme Gradient Boosting (`XGB`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2179dda",
   "metadata": {},
   "source": [
    "#### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4191030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_params_xgb_class = {\n",
    "              'booster': ['gbtree'],\n",
    "        'learning_rate': [0.3, 0.2, 0.1, 0.05],\n",
    "         'n_estimators': [100, 400, 800],\n",
    "            'max_depth': [6, 3, 9],\n",
    "     'min_child_weight': [0, 1.0, 1.1, 1.2],\n",
    "            'subsample': [1.0, 0.6, 0.8],\n",
    "     'colsample_bytree': [1.0, 0.75, 0.8],\n",
    "    'colsample_bylevel': [1.0, 0.75, 0.8],\n",
    "     'scale_pos_weight': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "xgb_class = XGBClassifier()\n",
    "\n",
    "random_search_xgb_class = RandomizedSearchCV(xgb_class,\n",
    "    random_params_xgb_class, n_iter=250, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "random_params_xgb_reg = {\n",
    "              'booster': ['gbtree'],\n",
    "        'learning_rate': [0.3, 0.2, 0.1, 0.05],\n",
    "         'n_estimators': [100, 400, 800],\n",
    "            'max_depth': [6, 3, 9],\n",
    "     'min_child_weight': [0, 1.0, 1.1, 1.2],\n",
    "            'subsample': [1.0, 0.6, 0.8],\n",
    "     'colsample_bytree': [1.0, 0.75, 0.8],\n",
    "    'colsample_bylevel': [1.0, 0.75, 0.8],\n",
    "     'scale_pos_weight': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "xgb_reg = XGBRegressor()\n",
    "\n",
    "random_search_xgb_reg = RandomizedSearchCV(xgb_reg,\n",
    "    random_params_xgb_reg, n_iter=250, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58862190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_xgb_class.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90966828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_xgb_reg.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d67b9a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for classification and regression:\n",
      "\n",
      "RandomizedSearchCV(cv=3,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                                           num_parallel_tree=None,\n",
      "                                           random_state=None, ...),\n",
      "                   n_iter=250, n_jobs=-1,\n",
      "                   param_distributions={'booster': ['gbtree'],\n",
      "                                        'colsample_bylevel': [1.0, 0.75, 0.8],\n",
      "                                        'colsample_bytree': [1.0, 0.75, 0.8],\n",
      "                                        'learning_rate': [0.3, 0.2, 0.1, 0.05],\n",
      "                                        'max_depth': [6, 3, 9],\n",
      "                                        'min_child_weight': [0, 1.0, 1.1, 1.2],\n",
      "                                        'n_estimators': [100, 400, 800],\n",
      "                                        'scale_pos_weight': [1.0, 1.1, 1.2],\n",
      "                                        'subsample': [1.0, 0.6, 0.8]},\n",
      "                   verbose=1)\n",
      "\n",
      "RandomizedSearchCV(cv=3,\n",
      "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
      "                                          callbacks=None,\n",
      "                                          colsample_bylevel=None,\n",
      "                                          colsample_bynode=None,\n",
      "                                          colsample_bytree=None, device=None,\n",
      "                                          early_stopping_rounds=None,\n",
      "                                          enable_categorical=False,\n",
      "                                          eval_metric=None, feature_types=None,\n",
      "                                          gamma=None, grow_policy=None,\n",
      "                                          importance_type=None,\n",
      "                                          interaction_constraints=None,\n",
      "                                          learning_rate=...\n",
      "                                          num_parallel_tree=None,\n",
      "                                          random_state=None, ...),\n",
      "                   n_iter=250, n_jobs=-1,\n",
      "                   param_distributions={'booster': ['gbtree'],\n",
      "                                        'colsample_bylevel': [1.0, 0.75, 0.8],\n",
      "                                        'colsample_bytree': [1.0, 0.75, 0.8],\n",
      "                                        'learning_rate': [0.3, 0.2, 0.1, 0.05],\n",
      "                                        'max_depth': [6, 3, 9],\n",
      "                                        'min_child_weight': [0, 1.0, 1.1, 1.2],\n",
      "                                        'n_estimators': [100, 400, 800],\n",
      "                                        'scale_pos_weight': [1.0, 1.1, 1.2],\n",
      "                                        'subsample': [1.0, 0.6, 0.8]},\n",
      "                   verbose=1)\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "Classifier:\n",
      "XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=0.75, colsample_bynode=None,\n",
      "              colsample_bytree=0.75, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=1.2, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "Accuracy: 0.6797359189149058\n",
      "\n",
      "Regressor:\n",
      "XGBRegressor(base_score=None, booster='gbtree', callbacks=None,\n",
      "             colsample_bylevel=1.0, colsample_bynode=None,\n",
      "             colsample_bytree=0.75, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=1.0, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=None, ...)\n",
      "Accuracy: 0.588526307622563\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "rfm_period 2/7:\n",
      "rfm_period 3/7:\n",
      "rfm_period 4/7:\n",
      "rfm_period 5/7:\n",
      "rfm_period 6/7:\n",
      "rfm_period 7/7:\n",
      "Prediction done!\n",
      "Classification accuracy: 0.6946736988225286\n",
      "Regression accuracy: 0.3514046686826277\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.42779698202450195\n"
     ]
    }
   ],
   "source": [
    "r2_results_xgb, r2_avg_xgb, hurdle_avg_xgb, xgb_predictions = grid_backtesting(\n",
    "    whole_data, random_search_xgb_class, random_search_xgb_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9532471",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>class_pred</th>\n",
       "      <th>class_pred_proba</th>\n",
       "      <th>reg_pred</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3865</td>\n",
       "      <td>405.0447</td>\n",
       "      <td>156.5474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>627.5455</td>\n",
       "      <td>173.5035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1949</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>473.1303</td>\n",
       "      <td>120.5982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>491.3715</td>\n",
       "      <td>256.3676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3075</td>\n",
       "      <td>473.1303</td>\n",
       "      <td>145.4930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2141</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6392</td>\n",
       "      <td>610.7352</td>\n",
       "      <td>390.3875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>392.4167</td>\n",
       "      <td>83.6507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>520.0519</td>\n",
       "      <td>319.4511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2868</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  class_pred  \\\n",
       "24321           7        18241            1    976.0900      0.0000   \n",
       "24322           7        18242            1   1538.4100      0.0000   \n",
       "24323           7        18245            0      0.0000      1.0000   \n",
       "24324           7        18246            0      0.0000      0.0000   \n",
       "24325           7        18248            1    286.5600      0.0000   \n",
       "24326           7        18250            0      0.0000      0.0000   \n",
       "24327           7        18252            0      0.0000      0.0000   \n",
       "24328           7        18257            1    627.2700      1.0000   \n",
       "24329           7        18260            0      0.0000      1.0000   \n",
       "24330           7        18262            0      0.0000      0.0000   \n",
       "24331           7        18263            0      0.0000      1.0000   \n",
       "24332           7        18265            1    312.9600      0.0000   \n",
       "24333           7        18268            0      0.0000      0.0000   \n",
       "24334           7        18270            0      0.0000      0.0000   \n",
       "24335           7        18272            1    372.2500      1.0000   \n",
       "24336           7        18273            1    102.0000      0.0000   \n",
       "24337           7        18280            0      0.0000      0.0000   \n",
       "24338           7        18281            0      0.0000      0.0000   \n",
       "24339           7        18283            1    130.9000      1.0000   \n",
       "24340           7        18287            0      0.0000      0.0000   \n",
       "\n",
       "       class_pred_proba  reg_pred  predicted_rev  \n",
       "24321            0.3865  405.0447       156.5474  \n",
       "24322            0.2765  627.5455       173.5035  \n",
       "24323            0.6013    0.0000         0.0000  \n",
       "24324            0.1949    0.0000         0.0000  \n",
       "24325            0.2549  473.1303       120.5982  \n",
       "24326            0.2113    0.0000         0.0000  \n",
       "24327            0.1905    0.0000         0.0000  \n",
       "24328            0.5217  491.3715       256.3676  \n",
       "24329            0.6252    0.0000         0.0000  \n",
       "24330            0.1817    0.0000         0.0000  \n",
       "24331            0.5176    0.0000         0.0000  \n",
       "24332            0.3075  473.1303       145.4930  \n",
       "24333            0.2233    0.0000         0.0000  \n",
       "24334            0.2141    0.0000         0.0000  \n",
       "24335            0.6392  610.7352       390.3875  \n",
       "24336            0.2132  392.4167        83.6507  \n",
       "24337            0.1842    0.0000         0.0000  \n",
       "24338            0.2346    0.0000         0.0000  \n",
       "24339            0.6143  520.0519       319.4511  \n",
       "24340            0.2868    0.0000         0.0000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3783cd1",
   "metadata": {},
   "source": [
    "#### Zero-Inflated Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a9329f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_params_xgb_zir = {\n",
    "              'classifier__booster': ['gbtree'],\n",
    "        'classifier__learning_rate': [0.3, 0.2, 0.1, 0.05],\n",
    "         'classifier__n_estimators': [100, 400, 800],\n",
    "            'classifier__max_depth': [6, 3, 9],\n",
    "     'classifier__min_child_weight': [0, 1.0, 1.1, 1.2],\n",
    "            'classifier__subsample': [1.0, 0.6, 0.8],\n",
    "     'classifier__colsample_bytree': [1.0, 0.75, 0.8],\n",
    "    'classifier__colsample_bylevel': [1.0, 0.75, 0.8],\n",
    "     'classifier__scale_pos_weight': [1.0, 1.1, 1.2],\n",
    "    \n",
    "               'regressor__booster': ['gbtree'],\n",
    "         'regressor__learning_rate': [0.3, 0.2, 0.1, 0.05],\n",
    "          'regressor__n_estimators': [100, 400, 800],\n",
    "             'regressor__max_depth': [6, 3, 9],\n",
    "      'regressor__min_child_weight': [0, 1.0, 1.1, 1.2],\n",
    "             'regressor__subsample': [1.0, 0.6, 0.8],\n",
    "      'regressor__colsample_bytree': [1.0, 0.75, 0.8],\n",
    "     'regressor__colsample_bylevel': [1.0, 0.75, 0.8],\n",
    "      'regressor__scale_pos_weight': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "xgb_zir = ZeroInflatedRegressor(\n",
    "    classifier=XGBClassifier(),\n",
    "    regressor =XGBRegressor()\n",
    ")\n",
    "\n",
    "random_search_xgb_zir = RandomizedSearchCV(\n",
    "    xgb_zir, random_params_xgb_zir, n_iter=250, cv=3, n_jobs=-1, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11b32a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ZeroInflatedRegressor(classifier=XGBClassifier(base_score=None, booster=None,\n",
       "                                               callbacks=None,\n",
       "                                               colsample_bylevel=None,\n",
       "                                               colsample_bynode=None,\n",
       "                                               colsample_bytree=None,\n",
       "                                               device=None,\n",
       "                                               early_stopping_rounds=None,\n",
       "                                               enable_categorical=False,\n",
       "                                               eval_metric=None,\n",
       "                                               feature_types=None, gamma=None,\n",
       "                                               grow_policy=None,\n",
       "                                               importance_type=None,\n",
       "                                               interaction_constraints=None,\n",
       "                                               learning_rate=...\n",
       "                                             grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None, max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             multi_strategy=None,\n",
       "                                             n_estimators=None, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             random_state=None, ...))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ZeroInflatedRegressor</label><div class=\"sk-toggleable__content\"><pre>ZeroInflatedRegressor(classifier=XGBClassifier(base_score=None, booster=None,\n",
       "                                               callbacks=None,\n",
       "                                               colsample_bylevel=None,\n",
       "                                               colsample_bynode=None,\n",
       "                                               colsample_bytree=None,\n",
       "                                               device=None,\n",
       "                                               early_stopping_rounds=None,\n",
       "                                               enable_categorical=False,\n",
       "                                               eval_metric=None,\n",
       "                                               feature_types=None, gamma=None,\n",
       "                                               grow_policy=None,\n",
       "                                               importance_type=None,\n",
       "                                               interaction_constraints=None,\n",
       "                                               learning_rate=...\n",
       "                                             grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None, max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             multi_strategy=None,\n",
       "                                             n_estimators=None, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             random_state=None, ...))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">regressor: XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ZeroInflatedRegressor(classifier=XGBClassifier(base_score=None, booster=None,\n",
       "                                               callbacks=None,\n",
       "                                               colsample_bylevel=None,\n",
       "                                               colsample_bynode=None,\n",
       "                                               colsample_bytree=None,\n",
       "                                               device=None,\n",
       "                                               early_stopping_rounds=None,\n",
       "                                               enable_categorical=False,\n",
       "                                               eval_metric=None,\n",
       "                                               feature_types=None, gamma=None,\n",
       "                                               grow_policy=None,\n",
       "                                               importance_type=None,\n",
       "                                               interaction_constraints=None,\n",
       "                                               learning_rate=...\n",
       "                                             grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None, max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             multi_strategy=None,\n",
       "                                             n_estimators=None, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             random_state=None, ...))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_xgb_zir.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca6af993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for ZIR (classification + regression):\n",
      "\n",
      "ZeroInflatedRegressor(classifier=XGBClassifier(base_score=None, booster=None,\n",
      "                                               callbacks=None,\n",
      "                                               colsample_bylevel=None,\n",
      "                                               colsample_bynode=None,\n",
      "                                               colsample_bytree=None,\n",
      "                                               device=None,\n",
      "                                               early_stopping_rounds=None,\n",
      "                                               enable_categorical=False,\n",
      "                                               eval_metric=None,\n",
      "                                               feature_types=None, gamma=None,\n",
      "                                               grow_policy=None,\n",
      "                                               importance_type=None,\n",
      "                                               interaction_constraints=None,\n",
      "                                               learning_rate=...\n",
      "                                             grow_policy=None,\n",
      "                                             importance_type=None,\n",
      "                                             interaction_constraints=None,\n",
      "                                             learning_rate=None, max_bin=None,\n",
      "                                             max_cat_threshold=None,\n",
      "                                             max_cat_to_onehot=None,\n",
      "                                             max_delta_step=None,\n",
      "                                             max_depth=None, max_leaves=None,\n",
      "                                             min_child_weight=None, missing=nan,\n",
      "                                             monotone_constraints=None,\n",
      "                                             multi_strategy=None,\n",
      "                                             n_estimators=None, n_jobs=None,\n",
      "                                             num_parallel_tree=None,\n",
      "                                             random_state=None, ...))\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "ZIR (Classifier+Regressor) :\n",
      "ZeroInflatedRegressor(classifier=XGBClassifier(base_score=None,\n",
      "                                               booster='gbtree', callbacks=None,\n",
      "                                               colsample_bylevel=1.0,\n",
      "                                               colsample_bynode=None,\n",
      "                                               colsample_bytree=0.75,\n",
      "                                               device=None,\n",
      "                                               early_stopping_rounds=None,\n",
      "                                               enable_categorical=False,\n",
      "                                               eval_metric=None,\n",
      "                                               feature_types=None, gamma=None,\n",
      "                                               grow_policy=None,\n",
      "                                               importance_type=None,\n",
      "                                               interaction_constraints=None,\n",
      "                                               learning_ra...\n",
      "                                             feature_types=None, gamma=None,\n",
      "                                             grow_policy=None,\n",
      "                                             importance_type=None,\n",
      "                                             interaction_constraints=None,\n",
      "                                             learning_rate=0.05, max_bin=None,\n",
      "                                             max_cat_threshold=None,\n",
      "                                             max_cat_to_onehot=None,\n",
      "                                             max_delta_step=None, max_depth=6,\n",
      "                                             max_leaves=None,\n",
      "                                             min_child_weight=1.1, missing=nan,\n",
      "                                             monotone_constraints=None,\n",
      "                                             multi_strategy=None,\n",
      "                                             n_estimators=100, n_jobs=None,\n",
      "                                             num_parallel_tree=None,\n",
      "                                             random_state=None, ...))\n",
      "Accuracy: 0.5767274017165437\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "rfm_period 2/7:\n",
      "rfm_period 3/7:\n",
      "rfm_period 4/7:\n",
      "rfm_period 5/7:\n",
      "rfm_period 6/7:\n",
      "rfm_period 7/7:\n",
      "Prediction done!\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.4177949348891855\n"
     ]
    }
   ],
   "source": [
    "hurdle_avg_xgb_zir, xgb_zir_predictions = (\n",
    "    grid_zeroinflated_backtesting(whole_data, random_search_xgb_zir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10efa57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>504.7548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>484.1864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>655.2324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>574.6534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>684.4023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>451.1741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  predicted_rev\n",
       "24321           7        18241            1    976.0900         0.0000\n",
       "24322           7        18242            1   1538.4100         0.0000\n",
       "24323           7        18245            0      0.0000       504.7548\n",
       "24324           7        18246            0      0.0000         0.0000\n",
       "24325           7        18248            1    286.5600         0.0000\n",
       "24326           7        18250            0      0.0000         0.0000\n",
       "24327           7        18252            0      0.0000         0.0000\n",
       "24328           7        18257            1    627.2700       484.1864\n",
       "24329           7        18260            0      0.0000       655.2324\n",
       "24330           7        18262            0      0.0000         0.0000\n",
       "24331           7        18263            0      0.0000       574.6534\n",
       "24332           7        18265            1    312.9600         0.0000\n",
       "24333           7        18268            0      0.0000         0.0000\n",
       "24334           7        18270            0      0.0000         0.0000\n",
       "24335           7        18272            1    372.2500       684.4023\n",
       "24336           7        18273            1    102.0000         0.0000\n",
       "24337           7        18280            0      0.0000         0.0000\n",
       "24338           7        18281            0      0.0000         0.0000\n",
       "24339           7        18283            1    130.9000       451.1741\n",
       "24340           7        18287            0      0.0000         0.0000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_zir_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3658cd0",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "Heres are the accuracies of the three model:\n",
    "\n",
    "| Methods       | Accuracy           | `RandomForest` | `LightGBM` | `XGB`    |\n",
    "|---------------|--------------------|----------------|------------|----------|\n",
    "| **Separated** | *Classification*   | `69.26%`       | `69.08%`   | `69.31%` |\n",
    "|               | *Regression*       | `57.23%`       | `50.88%`   | `43.27%` |\n",
    "| **Combined**  | *Class + Reg*      | `55.53%`       | `53.63%`   | `42.77%` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68809a",
   "metadata": {},
   "source": [
    "## Further Tuning: `RandomForest` by GridSearchCV\n",
    "\n",
    "We've encountered that amongst the three machine learning models, `RandomForest` has the best performance. Now we'll look for a better parameter by running all the parameter combinations through `GridSearchCV`. \n",
    "\n",
    "Here are the current best model for both methods:\n",
    "\n",
    "```\n",
    "# `grid_backtesting()`\n",
    "\n",
    "Classifier:\n",
    "RandomForestClassifier(\n",
    "    max_depth=7, min_samples_leaf=2, \n",
    "    min_samples_split=7, n_estimators=250)\n",
    "Accuracy: 0.6835577659413156   # validation\n",
    "\n",
    "Regressor:\n",
    "RandomForestRegressor(\n",
    "    max_depth=9, max_features='sqrt', \n",
    "    min_samples_leaf=2, min_samples_split=5)\n",
    "Accuracy: 0.6578797074055159   # validation\n",
    "\n",
    "Classification accuracy: 0.6926833642150635\n",
    "Regression accuracy: 0.5723393188749951\n",
    "Class + Reg R2 Score -- Hurdle Models: 0.5737303247965023\n",
    "\n",
    "\n",
    "# `grid_zeroinflated_backtesting()`\n",
    "\n",
    "ZIR (Classifier+Regressor) :\n",
    "ZeroInflatedRegressor(\n",
    "    classifier=RandomForestClassifier(\n",
    "        max_depth=9, min_samples_leaf=4,\n",
    "        min_samples_split=7, n_estimators=250),\n",
    "    regressor=RandomForestRegressor(\n",
    "        max_depth=5, max_features='sqrt',\n",
    "        min_samples_split=3, n_estimators=250))\n",
    "Accuracy: 0.6251959361544048\n",
    "\n",
    "Class + Reg R2 Score -- Hurdle Models: 0.5553419546639864\n",
    "\n",
    "```\n",
    "\n",
    "We're taking the whole combinations into account, hence for parameter in `ZeroInflatedRegressor` we'll have 1800000 parameters--way more a lot than previous ones. Therefore, we'll limit the options according to current best `RandomForest` model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65c557da",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_rf_class = {\n",
    "         'n_estimators': [100, 150, 200, 250, 300],\n",
    "            'max_depth': [2, 5, 7, 9, 10],\n",
    "    'min_samples_split': [2, 3, 5, 7],\n",
    "     'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_class = RandomForestClassifier()\n",
    "\n",
    "grid_search_rf_class = GridSearchCV(rf_class,\n",
    "    grid_params_rf_class, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_params_rf_reg = {\n",
    "         'n_estimators': [100, 150, 200, 250, 300],\n",
    "            'max_depth': [2, 5, 7, 9, 10],\n",
    "    'min_samples_split': [2, 3, 5, 7],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "         'max_features': ['sqrt', 6]\n",
    "}\n",
    "\n",
    "rf_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search_rf_reg = GridSearchCV(rf_reg,\n",
    "    grid_params_rf_reg, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7e859fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf_class.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87cdf023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf_reg.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1122d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for classification and regression:\n",
      "\n",
      "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
      "             param_grid={'max_depth': [2, 5, 7, 9, 10],\n",
      "                         'min_samples_leaf': [1, 2, 4],\n",
      "                         'min_samples_split': [2, 3, 5, 7],\n",
      "                         'n_estimators': [100, 150, 200, 250, 300]},\n",
      "             verbose=1)\n",
      "\n",
      "GridSearchCV(cv=3, estimator=RandomForestRegressor(), n_jobs=-1,\n",
      "             param_grid={'max_depth': [2, 5, 7, 9, 10],\n",
      "                         'max_features': ['sqrt', 6],\n",
      "                         'min_samples_leaf': [1, 2, 4],\n",
      "                         'min_samples_split': [2, 3, 5, 7],\n",
      "                         'n_estimators': [100, 150, 200, 250, 300]},\n",
      "             verbose=1)\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "Classifier:\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=2, min_samples_split=7,\n",
      "                       n_estimators=300)\n",
      "Accuracy: 0.6841392923977465\n",
      "\n",
      "Regressor:\n",
      "RandomForestRegressor(max_depth=7, max_features=6, min_samples_leaf=2,\n",
      "                      min_samples_split=3, n_estimators=200)\n",
      "Accuracy: 0.670901641203704\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "rfm_period 2/7:\n",
      "rfm_period 3/7:\n",
      "rfm_period 4/7:\n",
      "rfm_period 5/7:\n",
      "rfm_period 6/7:\n",
      "rfm_period 7/7:\n",
      "Prediction done!\n",
      "Classification accuracy: 0.6943469881177078\n",
      "Regression accuracy: 0.5722853430361522\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.592640516550141\n"
     ]
    }
   ],
   "source": [
    "(acc_avg_grid_rf, r2_avg_grid_rf, \n",
    " hurdle_avg_grid_rf, grid_rf_predictions) = grid_backtesting(\n",
    "    whole_data, grid_search_rf_class, grid_search_rf_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "583b1d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>class_pred</th>\n",
       "      <th>class_pred_proba</th>\n",
       "      <th>reg_pred</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>443.0491</td>\n",
       "      <td>191.8301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2764</td>\n",
       "      <td>532.5210</td>\n",
       "      <td>147.2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>497.5300</td>\n",
       "      <td>121.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1602</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5517</td>\n",
       "      <td>476.9606</td>\n",
       "      <td>263.1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1716</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5503</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3065</td>\n",
       "      <td>472.7809</td>\n",
       "      <td>144.9105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2795</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6413</td>\n",
       "      <td>657.0282</td>\n",
       "      <td>421.3302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1725</td>\n",
       "      <td>320.9572</td>\n",
       "      <td>55.3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1856</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>486.9715</td>\n",
       "      <td>276.7757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3298</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  class_pred  \\\n",
       "24321           7        18241            1    976.0900      0.0000   \n",
       "24322           7        18242            1   1538.4100      0.0000   \n",
       "24323           7        18245            0      0.0000      1.0000   \n",
       "24324           7        18246            0      0.0000      0.0000   \n",
       "24325           7        18248            1    286.5600      0.0000   \n",
       "24326           7        18250            0      0.0000      0.0000   \n",
       "24327           7        18252            0      0.0000      0.0000   \n",
       "24328           7        18257            1    627.2700      1.0000   \n",
       "24329           7        18260            0      0.0000      1.0000   \n",
       "24330           7        18262            0      0.0000      0.0000   \n",
       "24331           7        18263            0      0.0000      1.0000   \n",
       "24332           7        18265            1    312.9600      0.0000   \n",
       "24333           7        18268            0      0.0000      0.0000   \n",
       "24334           7        18270            0      0.0000      0.0000   \n",
       "24335           7        18272            1    372.2500      1.0000   \n",
       "24336           7        18273            1    102.0000      0.0000   \n",
       "24337           7        18280            0      0.0000      0.0000   \n",
       "24338           7        18281            0      0.0000      0.0000   \n",
       "24339           7        18283            1    130.9000      1.0000   \n",
       "24340           7        18287            0      0.0000      0.0000   \n",
       "\n",
       "       class_pred_proba  reg_pred  predicted_rev  \n",
       "24321            0.4330  443.0491       191.8301  \n",
       "24322            0.2764  532.5210       147.2144  \n",
       "24323            0.6116    0.0000         0.0000  \n",
       "24324            0.1829    0.0000         0.0000  \n",
       "24325            0.2443  497.5300       121.5423  \n",
       "24326            0.2140    0.0000         0.0000  \n",
       "24327            0.1602    0.0000         0.0000  \n",
       "24328            0.5517  476.9606       263.1363  \n",
       "24329            0.6386    0.0000         0.0000  \n",
       "24330            0.1716    0.0000         0.0000  \n",
       "24331            0.5503    0.0000         0.0000  \n",
       "24332            0.3065  472.7809       144.9105  \n",
       "24333            0.2795    0.0000         0.0000  \n",
       "24334            0.2065    0.0000         0.0000  \n",
       "24335            0.6413  657.0282       421.3302  \n",
       "24336            0.1725  320.9572        55.3705  \n",
       "24337            0.1856    0.0000         0.0000  \n",
       "24338            0.2243    0.0000         0.0000  \n",
       "24339            0.5684  486.9715       276.7757  \n",
       "24340            0.3298    0.0000         0.0000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_predictions[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d82e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options in grid_params for ZIR are\n",
    "## decreased for time efficiency\n",
    "\n",
    "### Total params combinations: 2592\n",
    "grid_params_rf_zir = {\n",
    "         'classifier__n_estimators': [150, 300],\n",
    "            'classifier__max_depth': [2, 5, 7],\n",
    "    'classifier__min_samples_split': [2, 3, 5],\n",
    "     'classifier__min_samples_leaf': [1, 2, 4],\n",
    "\n",
    "          'regressor__n_estimators': [150, 300],\n",
    "             'regressor__max_depth': [2, 5, 7],\n",
    "     'regressor__min_samples_split': [2, 3, 5],\n",
    "      'regressor__min_samples_leaf': [1, 2, 4],\n",
    "          'regressor__max_features': ['sqrt', 6]\n",
    "}\n",
    "\n",
    "rf_zir = ZeroInflatedRegressor(\n",
    "    classifier=RandomForestClassifier(),\n",
    "    regressor =RandomForestRegressor()\n",
    ")\n",
    "\n",
    "grid_search_rf_zir = GridSearchCV(\n",
    "    rf_zir, grid_params_rf_zir, cv=3, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1183c392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
       "                      regressor=RandomForestRegressor())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ZeroInflatedRegressor</label><div class=\"sk-toggleable__content\"><pre>ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
       "                      regressor=RandomForestRegressor())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">regressor: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
       "                      regressor=RandomForestRegressor())"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf_zir.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2d25c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for ZIR (classification + regression):\n",
      "\n",
      "ZeroInflatedRegressor(classifier=RandomForestClassifier(),\n",
      "                      regressor=RandomForestRegressor())\n",
      "\n",
      "rfm_period 1/4:\n",
      "Fitting 3 folds for each of 5832 candidates, totalling 17496 fits\n",
      "rfm_period 2/4:\n",
      "Fitting 3 folds for each of 5832 candidates, totalling 17496 fits\n",
      "rfm_period 3/4:\n",
      "Fitting 3 folds for each of 5832 candidates, totalling 17496 fits\n",
      "rfm_period 4/4:\n",
      "Fitting 3 folds for each of 5832 candidates, totalling 17496 fits\n",
      "\n",
      "Best params for the model: \n",
      "\n",
      "ZIR (Classifier+Regressor) :\n",
      "ZeroInflatedRegressor(classifier=RandomForestClassifier(max_depth=5,\n",
      "                                                        min_samples_leaf=4,\n",
      "                                                        n_estimators=300),\n",
      "                      regressor=RandomForestRegressor(max_depth=5,\n",
      "                                                      max_features=6,\n",
      "                                                      min_samples_leaf=2,\n",
      "                                                      n_estimators=150))\n",
      "Accuracy: 0.6341861883066895\n",
      "\n",
      "Cross-validation done!\n",
      "\n",
      "Fitting best parameters into the next periods:\n",
      "rfm_period 1/7:\n",
      "rfm_period 2/7:\n",
      "rfm_period 3/7:\n",
      "rfm_period 4/7:\n",
      "rfm_period 5/7:\n",
      "rfm_period 6/7:\n",
      "rfm_period 7/7:\n",
      "Prediction done!\n",
      "Class + Reg R2 Score -- Hurdle Models: 0.5421405377273045\n"
     ]
    }
   ],
   "source": [
    "grid_hurdle_avg_rf_zir, grid_rf_zir_predictions = (\n",
    "    grid_zeroinflated_backtesting(whole_data, grid_search_rf_zir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79e2c23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_period</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>is_returned</th>\n",
       "      <th>target_rev</th>\n",
       "      <th>predicted_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24321</th>\n",
       "      <td>7</td>\n",
       "      <td>18241</td>\n",
       "      <td>1</td>\n",
       "      <td>976.0900</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24322</th>\n",
       "      <td>7</td>\n",
       "      <td>18242</td>\n",
       "      <td>1</td>\n",
       "      <td>1538.4100</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>7</td>\n",
       "      <td>18245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>535.6987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>7</td>\n",
       "      <td>18246</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24325</th>\n",
       "      <td>7</td>\n",
       "      <td>18248</td>\n",
       "      <td>1</td>\n",
       "      <td>286.5600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>7</td>\n",
       "      <td>18250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>7</td>\n",
       "      <td>18252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>7</td>\n",
       "      <td>18257</td>\n",
       "      <td>1</td>\n",
       "      <td>627.2700</td>\n",
       "      <td>485.0395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>7</td>\n",
       "      <td>18260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>688.5816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>7</td>\n",
       "      <td>18262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>7</td>\n",
       "      <td>18263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>550.6290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>7</td>\n",
       "      <td>18265</td>\n",
       "      <td>1</td>\n",
       "      <td>312.9600</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>7</td>\n",
       "      <td>18268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24334</th>\n",
       "      <td>7</td>\n",
       "      <td>18270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>7</td>\n",
       "      <td>18272</td>\n",
       "      <td>1</td>\n",
       "      <td>372.2500</td>\n",
       "      <td>693.0778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>7</td>\n",
       "      <td>18273</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24337</th>\n",
       "      <td>7</td>\n",
       "      <td>18280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>7</td>\n",
       "      <td>18281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24339</th>\n",
       "      <td>7</td>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>130.9000</td>\n",
       "      <td>495.9837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>7</td>\n",
       "      <td>18287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rfm_period  Customer ID  is_returned  target_rev  predicted_rev\n",
       "24321           7        18241            1    976.0900         0.0000\n",
       "24322           7        18242            1   1538.4100         0.0000\n",
       "24323           7        18245            0      0.0000       535.6987\n",
       "24324           7        18246            0      0.0000         0.0000\n",
       "24325           7        18248            1    286.5600         0.0000\n",
       "24326           7        18250            0      0.0000         0.0000\n",
       "24327           7        18252            0      0.0000         0.0000\n",
       "24328           7        18257            1    627.2700       485.0395\n",
       "24329           7        18260            0      0.0000       688.5816\n",
       "24330           7        18262            0      0.0000         0.0000\n",
       "24331           7        18263            0      0.0000       550.6290\n",
       "24332           7        18265            1    312.9600         0.0000\n",
       "24333           7        18268            0      0.0000         0.0000\n",
       "24334           7        18270            0      0.0000         0.0000\n",
       "24335           7        18272            1    372.2500       693.0778\n",
       "24336           7        18273            1    102.0000         0.0000\n",
       "24337           7        18280            0      0.0000         0.0000\n",
       "24338           7        18281            0      0.0000         0.0000\n",
       "24339           7        18283            1    130.9000       495.9837\n",
       "24340           7        18287            0      0.0000         0.0000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_zir_predictions[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50aac5e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We've predicted customer revenue by three models resulting in a `RandomForest` model. The accuracy on both `RandomizedSearchCV` and `GridSearchCV` are shown below.\n",
    "\n",
    "| Methods       | RF Accuracy  | RandomizedSearchCV | GridSearchCV |\n",
    "|---------------|--------------------|---------|----------|\n",
    "| **Separated** | *Classification*   | `69.26%`| `69.43%` |\n",
    "|               | *Regression*       | `57.23%`| `57.22%` | \n",
    "| **Combined**  | *Class + Reg*      | `55.53%`| `54.21%` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
